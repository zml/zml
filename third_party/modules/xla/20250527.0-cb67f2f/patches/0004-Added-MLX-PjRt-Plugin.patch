diff --git a/xla/pjrt/plugin/mlx/BUILD b/xla/pjrt/plugin/mlx/BUILD
new file mode 100644
index 0000000000..8255851b7f
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/BUILD
@@ -0,0 +1,243 @@
+load("//xla:xla.default.bzl", "xla_cc_binary", "xla_cc_test")
+
+package(
+    # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+cc_library(
+    name = "buffer",
+    srcs = ["buffer.cc"],
+    hdrs = ["buffer.h"],
+    deps = [
+        ":logging",
+        ":utils",
+        "//xla:literal",
+        "//xla:shape_util",
+        "//xla:util",
+        "//xla/hlo/translate/hlo_to_mhlo:hlo_utils",
+        "//xla/hlo/translate/mhlo_to_hlo:literal_exporter",
+        "//xla/hlo/translate/mhlo_to_hlo:type_to_shape",
+        "//xla/pjrt:pjrt_client",
+        "//xla/pjrt:pjrt_compiler",
+        "//xla/pjrt:pjrt_future",
+        "@com_google_absl//absl/functional:any_invocable",
+        "@com_google_absl//absl/log",
+        "@com_google_absl//absl/status:statusor",
+        "@com_google_absl//absl/types:span",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:AsmParser",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+    ],
+)
+
+cc_library(
+    name = "compile",
+    srcs = ["compile.cc"],
+    hdrs = ["compile.h"],
+    deps = [
+        "@mlx//:mlx",
+        ":buffer",
+        ":logging",
+        ":utils",
+        "//xla/hlo/builder:xla_computation",
+        "//xla/hlo/translate:stablehlo",
+        "//xla/mlir/utils:error_util",
+        "//xla/mlir_hlo:mhlo_passes",
+        "//xla/mlir_hlo:stablehlo_extension_passes",
+        "//xla/mlir/utils:type_util",
+        # "//xla/pjrt:mlir_to_hlo",
+        "//xla/pjrt:pjrt_client",
+        "//xla/pjrt:pjrt_executable",
+        "//xla/pjrt:pjrt_future",
+        "//xla/service:computation_placer_hdr",
+        "@com_google_absl//absl/log",
+        "@com_google_absl//absl/status",
+        "@com_google_absl//absl/status:statusor",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:AsmParser",
+        "@llvm-project//mlir:BytecodeWriter",
+        "@llvm-project//mlir:DataLayoutInterfaces",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Parser",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:Transforms",
+        "@stablehlo//:reference_api",
+        "@stablehlo//:stablehlo_passes_optimization",
+        "@tsl//tsl/platform:statusor",
+    ],
+)
+
+
+cc_library(
+    name = "executable",
+    srcs = ["executable.cc"],
+    hdrs = ["executable.h"],
+    deps = [
+        "@mlx//:mlx",
+        ":buffer",
+        ":compile",
+        ":logging",
+        ":utils",
+        "//xla/hlo/builder:xla_computation",
+        "//xla/hlo/translate:stablehlo",
+        "//xla/mlir/utils:error_util",
+        "//xla/mlir_hlo:mhlo_passes",
+        "//xla/mlir_hlo:stablehlo_extension_passes",
+        "//xla/mlir/utils:type_util",
+        # "//xla/pjrt:mlir_to_hlo",
+        "//xla/pjrt:pjrt_client",
+        "//xla/pjrt:pjrt_executable",
+        "//xla/pjrt:pjrt_future",
+        "//xla/service:computation_placer_hdr",
+        "@com_google_absl//absl/log",
+        "@com_google_absl//absl/status",
+        "@com_google_absl//absl/status:statusor",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:AsmParser",
+        "@llvm-project//mlir:BytecodeWriter",
+        "@llvm-project//mlir:DataLayoutInterfaces",
+        "@llvm-project//mlir:FuncDialect",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Parser",
+        "@llvm-project//mlir:Pass",
+        "@llvm-project//mlir:Support",
+        "@llvm-project//mlir:Transforms",
+        "@stablehlo//:reference_api",
+        "@stablehlo//:stablehlo_passes_optimization",
+        "@tsl//tsl/platform:statusor",
+    ],
+)
+
+cc_library(
+    name = "device",
+    srcs = ["device.cc"],
+    hdrs = ["device.h"],
+    deps = [
+        ":logging",
+        "//xla:util",
+        "//xla/pjrt:pjrt_client",
+        "//xla/pjrt:pjrt_common",
+        "//xla/pjrt:pjrt_compiler",
+        "//xla/pjrt:pjrt_device_description",
+        "@com_google_absl//absl/status:statusor",
+        "@com_google_absl//absl/strings:str_format",
+        "@com_google_absl//absl/strings:string_view",
+        "@com_google_absl//absl/types:span",
+        "@mlx//:mlx",
+    ],
+)
+
+cc_library(
+    name = "client_cpp_pjrt",
+    srcs = [
+        "client_cpp_pjrt.cc",
+    ],
+    hdrs = [
+        "client_cpp_pjrt.h",
+    ],
+    deps = [
+        # Deps used in example plugin
+        "//xla/pjrt:pjrt_client",
+        "//xla/pjrt:pjrt_compiler",
+        "@com_google_absl//absl/strings:string_view",
+        "@com_google_absl//absl/types:span",
+        "@tsl//tsl/platform:fingerprint",
+        # Deps used for mlx plugin
+        ":buffer",
+        ":device",
+        ":executable",
+        ":logging",
+        ":utils",
+        "//xla:literal",
+        "//xla:shape_util",
+        "//xla:util",
+        "//xla/pjrt:host_memory_spaces",
+        "//xla/pjrt:pjrt_common",
+        "//xla/tsl/framework:allocator",
+        "@com_google_absl//absl/status:statusor",
+        "@com_google_absl//absl/strings:str_format",
+        "@mlx//:mlx",
+        "@tsl//tsl/platform:statusor"
+    ],
+)
+
+cc_library(
+    name = "client_c_pjrt",
+    srcs = [
+        "client_c_pjrt.cc",
+    ],
+    hdrs = ["client_c_pjrt.h"],
+    deps = [
+        ":client_cpp_pjrt",
+        "//xla/pjrt:pjrt_client",
+        "//xla/pjrt/c:pjrt_c_api_hdrs",
+        "//xla/pjrt/c:pjrt_c_api_layouts_extension_hdrs",
+        "//xla/pjrt/c:pjrt_c_api_wrapper_impl",
+        "@com_google_absl//absl/status",
+    ],
+    alwayslink = 1,
+)
+
+cc_library(
+    name = "logging",
+    srcs = ["logging.cc"],
+    hdrs = ["logging.h"],
+    deps = [
+        "@com_google_absl//absl/base:log_severity",
+        "@com_google_absl//absl/log",
+        "@com_google_absl//absl/log:globals",
+        "@llvm-project//llvm:Support",
+        "@llvm-project//mlir:AsmParser",
+        "@llvm-project//mlir:DataLayoutInterfaces",
+        "@llvm-project//mlir:IR",
+    ],
+)
+
+cc_library(
+    name = "utils",
+    srcs = ["utils.cc"],
+    hdrs = ["utils.h"],
+    deps = [
+        "@com_google_absl//absl/status:statusor",
+        "@com_google_absl//absl/types:span",
+        "@llvm-project//mlir:IR",
+        "@llvm-project//mlir:Support",
+        "@mlx//:mlx",
+        "@stablehlo//:reference_api",
+        "//xla/mlir/utils:type_util",
+        "//xla:literal",
+        "//xla:shape_util",
+        "//xla:util",
+        ":logging",
+    ],
+)
+
+xla_cc_test(
+    name = "plugin_pjrt_test",
+    srcs = ["plugin_pjrt_test.cc"],
+    deps = [
+        ":client_c_pjrt",
+        ":client_cpp_pjrt",
+        "//xla/pjrt:pjrt_client_test_common",
+        "//xla/pjrt/c:pjrt_c_api_test_common",
+        "//xla/pjrt/c:pjrt_c_api_wrapper_impl",
+        "@com_google_googletest//:gtest_main",
+    ],
+)
+
+xla_cc_binary(
+    name = "stablehlo_mlx_plugin.so",
+    linkshared = True,
+    linkopts = [
+        "-Wl,-exported_symbol,_GetPjrtApi",
+        "-Wl,-undefined,error",
+    ],
+    deps = [
+        ":client_c_pjrt",
+    ],
+)
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/buffer.cc b/xla/pjrt/plugin/mlx/buffer.cc
new file mode 100644
index 0000000000..f87671b3d5
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/buffer.cc
@@ -0,0 +1,372 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/pjrt/plugin/mlx/buffer.h"
+
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "absl/functional/any_invocable.h"
+#include "absl/log/log.h"
+#include "absl/status/statusor.h"
+#include "absl/types/span.h"
+#include "llvm/Support/Casting.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/AsmParser/AsmParser.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/Builders.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/BuiltinTypeInterfaces.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlx/mlx.h"
+#include "mlir/Support/DebugStringHelper.h"
+#include "xla/hlo/translate/hlo_to_mhlo/hlo_utils.h"
+#include "xla/hlo/translate/mhlo_to_hlo/literal_exporter.h"
+#include "xla/hlo/translate/mhlo_to_hlo/type_to_shape.h"
+#include "xla/literal.h"
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/pjrt/pjrt_compiler.h"
+#include "xla/pjrt/pjrt_future.h"
+#include "xla/pjrt/plugin/mlx/logging.h"
+#include "xla/pjrt/plugin/mlx/utils.h"
+#include "xla/shape.h"
+#include "xla/util.h"
+
+namespace mx = mlx::core;
+namespace mlir::stablehlo {
+
+using xla::MutableLiteralBase;
+using xla::PjRtBuffer;
+using xla::PjRtClient;
+using xla::PjRtDevice;
+using xla::PjRtFuture;
+using xla::PjRtMemorySpace;
+using xla::PjRtPlatformId;
+using xla::Shape;
+
+#define UNIMPLEMENTED(name) \
+  xla::Unimplemented("MlxPjrtBuffer::" #name " is not implemented")
+
+class MlxPjrtBuffer : public PjRtBuffer {
+ public:
+  MlxPjrtBuffer(mx::array array, const Shape& shape,
+                PjRtMemorySpace* memory_space)
+      : xla::PjRtBuffer(),
+        context_(),
+        buffer_(),
+        array_(array),
+        shape_(shape),
+        memory_space_(memory_space) {
+    // TRACE_ME_MEMBER;
+  }
+
+  class MlirClonedExternalReference : public ExternalReference {
+   public:
+    explicit MlirClonedExternalReference(PjRtBuffer* buffer,
+                                         PjRtMemorySpace* memory_space)
+        : buffer_() {
+      // TRACE_ME_MEMBER;
+      auto mlir_buffer = GetAttributeFromBuffer(buffer);
+      if (!mlir_buffer.ok()) {
+        LOG(ERROR) << "Could not get attribute from buffer: "
+                   << mlir_buffer.status();
+      }
+
+      auto array = GetArrayFromBuffer(buffer);
+      if (!array.ok()) {
+        LOG(ERROR) << "Could not get attribute from buffer: " << array.status();
+      }
+      buffer_ = CreateMlirBufferFromAttribute(
+          array.value(), mlir_buffer.value(), memory_space);
+      data_ptr_ = (void*)mlir_buffer.value().getRawData().data();
+    }
+
+   private:
+    std::unique_ptr<PjRtBuffer> buffer_;
+  };
+
+  // All buffers are managed by the MLIR Context
+  ~MlxPjrtBuffer() override = default;
+
+  MlxPjrtBuffer(const MlxPjrtBuffer&) = delete;
+  MlxPjrtBuffer(MlxPjrtBuffer&&) = delete;
+  MlxPjrtBuffer& operator=(const MlxPjrtBuffer&) = delete;
+  MlxPjrtBuffer& operator=(MlxPjrtBuffer&&) = delete;
+
+  static std::unique_ptr<MlxPjrtBuffer> CreateFromLiteral(
+      mx::array array, const xla::LiteralSlice& literal,
+      xla::PjRtMemorySpace* memory_space) {
+    TRACE_ME;
+    auto buffer =
+        std::make_unique<MlxPjrtBuffer>(array, literal.shape(), memory_space);
+    mlir::Builder builder(&buffer->context_);
+    auto attr = xla::CreateDenseElementsAttrFromLiteral(literal, builder);
+    if (!attr.ok()) {
+      LOG(ERROR) << "Could not create dense elements attr from literal: "
+                 << attr.status();
+      return nullptr;
+    }
+    buffer->buffer_ = attr.value();
+    return buffer;
+  }
+
+  static std::unique_ptr<MlxPjrtBuffer> CreateFromAttribute(
+      mx::array array, DenseElementsAttr attr,
+      xla::PjRtMemorySpace* memory_space) {
+    TRACE_ME;
+
+    // MLIR type to xla shape:
+    Shape shape = xla::TypeToShape(attr.getType());
+    auto buffer = std::make_unique<MlxPjrtBuffer>(array, shape, memory_space);
+    buffer->buffer_ = CloneIntoContext(attr, buffer->context_);
+    return buffer;
+  }
+
+  const Shape& on_device_shape() const override {
+    // TRACE_ME_MEMBER;
+    return shape_;
+  }
+  absl::StatusOr<Shape> logical_on_device_shape() override {
+    // TRACE_ME_MEMBER;
+    return shape_;
+  }
+
+  PjRtPlatformId platform_id() const {
+    // TRACE_ME_MEMBER;
+    return client()->platform_id();
+  }
+
+  absl::string_view platform_name() const {
+    // TRACE_ME_MEMBER;
+    return client()->platform_name();
+  }
+
+  bool IsEmptyTuple() const {
+    // TRACE_ME_MEMBER;
+    return shape_.IsTuple() && shape_.tuple_shapes().empty();
+  }
+
+  // Buffer knows device + client per older design, should only need
+  // memory_space.
+  PjRtMemorySpace* memory_space() const override {
+    // TRACE_ME_MEMBER;
+    return memory_space_;
+  }
+  PjRtDevice* device() const override {
+    // TRACE_ME_MEMBER;
+    return memory_space_->devices().front();
+  }
+  PjRtClient* client() const override {
+    // TRACE_ME_MEMBER;
+    return memory_space_->client();
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer::ExternalReference>>
+  AcquireExternalReference() override {
+    // TRACE_ME_MEMBER;
+    return std::make_unique<MlirClonedExternalReference>(this, memory_space_);
+  }
+
+  xla::PjRtFuture<> ToLiteral(xla::MutableLiteralBase* literal) override {
+    // TRACE_ME_MEMBER;
+    if (IsEmptyTuple()) {
+      return PjRtFuture<>(
+          xla::InvalidArgument("ToLiteral called on empty tuple"));
+    }
+
+    absl::StatusOr<xla::Literal> to_copy =
+        mhlo::CreateLiteralFromAttribute(buffer_, {});
+    if (!to_copy.ok()) return PjRtFuture<>(to_copy.status());
+
+    // Synchronous! To make async, make the buffer, start the copy, and return a
+    // future that is ready when the copy is done.
+    auto status = literal->CopyFrom(to_copy.value());
+    if (!status.ok()) return PjRtFuture<>(status);
+    return PjRtFuture<>(absl::OkStatus());
+  }
+
+  PjRtFuture<> LazyToLiteral(
+      absl::AnyInvocable<absl::StatusOr<MutableLiteralBase*>() &&> generator)
+      override {
+    // TRACE_ME_MEMBER;
+    auto buffer = std::move(generator)();
+    if (!buffer.ok()) return PjRtFuture<>(buffer.status());
+    return ToLiteral(buffer.value());
+  }
+
+  absl::StatusOr<size_t> GetOnDeviceSizeInBytes() const override {
+    // This is needed by AcquireExternalReference, for framework figuring out
+    // how to read the underlying buffer data.
+    // TRACE_ME_MEMBER;
+    if (!buffer_) return 0;
+    return buffer_.getRawData().size();
+  }
+
+  PjRtFuture<> CopyRawToHost(void* dst, int64_t offset,
+                             int64_t transfer_size) override {
+    // TRACE_ME_MEMBER;
+    return PjRtFuture<>(UNIMPLEMENTED(CopyRawToHost));
+  }
+
+  absl::StatusOr<std::unique_ptr<ExternalReference>>
+  ReleaseDeviceMemoryOwnership(bool wait_for_operations_to_complete) override {
+    // TRACE_ME_MEMBER;
+    auto external_ref = AcquireExternalReference();
+    Delete();
+    return external_ref;
+  }
+
+  // Remove the buffer if deleted.
+  // Note: deleted and uninitialized appear the same in this scenario.
+  // Consider changing to mlir::NoneType when deleted.
+  void Delete() override {
+    // TRACE_ME_MEMBER;
+    buffer_ = {};
+    is_deleted_ = true;
+  }
+
+  bool IsDeleted() override {
+    // TRACE_ME_MEMBER;
+    return !buffer_ && is_deleted_;
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> CopyToDevice(
+      xla::PjRtDevice* dst_device) {
+    // TRACE_ME_MEMBER;
+    return CopyToMemorySpace(
+        dst_device->default_memory_space().value_or(nullptr));
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> CopyToMemorySpace(
+      xla::PjRtMemorySpace* dst_memory_space) override {
+    // TRACE_ME_MEMBER;
+    return CreateMlirBufferFromAttribute(array_, buffer_, dst_memory_space);
+  }
+
+  void CopyToRemoteDevice(
+      xla::PjRtFuture<std::string> serialized_descriptor,
+      xla::PjRtBuffer::RemoteSendCallback on_done) override {
+    // TRACE_ME_MEMBER;
+    on_done(UNIMPLEMENTED(CopyToRemoteDevice), false);
+  }
+
+  xla::PjRtFuture<> GetReadyFuture() override {
+    // TRACE_ME_MEMBER;
+    // Synchronous! To make async, have the device make a buffer with a ready
+    // future that is ready when the computation is done / buffer is ready.
+    return PjRtFuture<>(absl::OkStatus());
+  }
+
+  bool IsOnCpu() const override {
+    // If buffer is on CPU, it will be shared with framework via
+    // GetExternalReference, lse it is copied back to host.
+    // Since we are using reference interpreter, we are running on CPU in a
+    // shared memory space.
+    // TRACE_ME_MEMBER;
+    return false;
+  }
+
+  mlir::DenseElementsAttr GetBufferAttribute() const { return buffer_; }
+
+  mx::array GetArray() { return array_; }
+
+ private:
+  MLIRContext context_;
+  mlir::DenseElementsAttr buffer_;
+  bool is_deleted_ = false;
+  mx::array array_;
+
+  xla::Shape shape_;
+  PjRtMemorySpace* memory_space_;
+};
+
+std::unique_ptr<xla::PjRtBuffer> CreateMlirBufferFromMlxArray(
+    mx::array array, xla::PjRtMemorySpace* memory_space) {
+  TRACE_ME;
+  std::vector<int64_t> span_shape(array.shape().begin(), array.shape().end());
+  auto shape = xla::ShapeUtil::MakeShape(
+      utils::dtype::asXlaPrimitiveType(array.dtype()),
+      absl::Span<const int64_t>(span_shape.data(), span_shape.size()));
+  auto literal = xla::BorrowingLiteral(array.data<char>(), shape);
+  return MlxPjrtBuffer::CreateFromLiteral(array, literal, memory_space);
+}
+
+std::unique_ptr<PjRtBuffer> CreateMlirBufferFromLiteral(
+    const xla::LiteralSlice& literal, xla::PjRtMemorySpace* memory_space) {
+  TRACE_ME;
+  auto maybe_array = utils::array::fromHostLiteral(literal);
+  return MlxPjrtBuffer::CreateFromLiteral(maybe_array.value(), literal,
+                                          memory_space);
+}
+
+std::unique_ptr<PjRtBuffer> CreateMlirBufferFromAttribute(
+    mx::array array, DenseElementsAttr attr,
+    xla::PjRtMemorySpace* memory_space) {
+  TRACE_ME;
+  return MlxPjrtBuffer::CreateFromAttribute(array, attr, memory_space);
+}
+
+std::unique_ptr<PjRtBuffer> CreateMlirBufferUninitialized(
+    const xla::Shape& shape, PjRtMemorySpace* memory_space) {
+  TRACE_ME;
+  // TODO (@cryptodeal): C API doesn't implement this, but
+  // we'll want to ensure when the Buffer is initialized,
+  // the resulting array is correct shape/dtype.
+
+  // Init empty array of dtype
+  auto array = mx::array({});
+  return std::make_unique<MlxPjrtBuffer>(array, shape, memory_space);
+}
+
+absl::StatusOr<mlir::DenseElementsAttr> GetAttributeFromBuffer(
+    xla::PjRtBuffer* buffer) {
+  TRACE_ME;
+  if (buffer == nullptr || buffer->IsDeleted()) {
+    return xla::InvalidArgument("Buffer is null or deleted");
+  }
+  auto mlir_buffer = dynamic_cast<MlxPjrtBuffer*>(buffer);
+  if (mlir_buffer->IsDeleted()) {
+    return xla::InvalidArgument("Buffer is deleted");
+  }
+  if (mlir_buffer == nullptr) {
+    return xla::InvalidArgument("Buffer is not a MlxPjrtBuffer");
+  }
+  return mlir_buffer->GetBufferAttribute();
+}
+
+absl::StatusOr<mx::array> GetArrayFromBuffer(xla::PjRtBuffer* buffer) {
+  TRACE_ME;
+  if (buffer == nullptr || buffer->IsDeleted()) {
+    return xla::InvalidArgument("Buffer is null or deleted");
+  }
+  auto mlir_buffer = dynamic_cast<MlxPjrtBuffer*>(buffer);
+  if (mlir_buffer->IsDeleted()) {
+    return xla::InvalidArgument("Buffer is deleted");
+  }
+  if (mlir_buffer == nullptr) {
+    return xla::InvalidArgument("Buffer is not a MlxPjrtBuffer");
+  }
+
+  return mlir_buffer->GetArray();
+}
+
+DenseElementsAttr CloneIntoContext(DenseElementsAttr attr,
+                                   MLIRContext& context) {
+  Type type = mlir::parseType(mlir::debugString(attr.getType()), &context);
+  return DenseElementsAttr::getFromRawBuffer(llvm::cast<ShapedType>(type),
+                                             attr.getRawData());
+}
+
+}  // namespace mlir::stablehlo
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/buffer.h b/xla/pjrt/plugin/mlx/buffer.h
new file mode 100644
index 0000000000..1740f3c779
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/buffer.h
@@ -0,0 +1,43 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_PJRT_PLUGIN_STABLEHLO_MLX_BUFFER_H_
+#define XLA_PJRT_PLUGIN_STABLEHLO_MLX_BUFFER_H_
+
+#include <memory>
+
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlx/mlx.h"
+#include "xla/literal.h"
+#include "xla/pjrt/pjrt_client.h"
+
+namespace mx = mlx::core;
+
+namespace mlir::stablehlo {
+std::unique_ptr<xla::PjRtBuffer> CreateMlirBufferFromMlxArray(
+    mx::array array, xla::PjRtMemorySpace* memory_space);
+std::unique_ptr<xla::PjRtBuffer> CreateMlirBufferFromLiteral(
+    const xla::LiteralSlice& literal, xla::PjRtMemorySpace* memory_space);
+std::unique_ptr<xla::PjRtBuffer> CreateMlirBufferFromAttribute(
+    mx::array array, DenseElementsAttr attribute,
+    xla::PjRtMemorySpace* memory_space);
+std::unique_ptr<xla::PjRtBuffer> CreateMlirBufferUninitialized(
+    const xla::Shape& shape, xla::PjRtMemorySpace* memory_space);
+absl::StatusOr<DenseElementsAttr> GetAttributeFromBuffer(
+    xla::PjRtBuffer* buffer);
+absl::StatusOr<mx::array> GetArrayFromBuffer(xla::PjRtBuffer* buffer);
+DenseElementsAttr CloneIntoContext(DenseElementsAttr attr,
+                                   MLIRContext& context);
+
+}  // namespace mlir::stablehlo
+
+#endif  // XLA_PJRT_PLUGIN_STABLEHLO_MLX_BUFFER_H_
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/client_c_pjrt.cc b/xla/pjrt/plugin/mlx/client_c_pjrt.cc
new file mode 100644
index 0000000000..6a0fcbcbcf
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/client_c_pjrt.cc
@@ -0,0 +1,66 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/pjrt/plugin/mlx/client_c_pjrt.h"
+
+#include <cstdio>
+#include <memory>
+#include <utility>
+
+#include "absl/status/status.h"
+#include "xla/pjrt/c/pjrt_c_api.h"
+#include "xla/pjrt/c/pjrt_c_api_layouts_extension.h"
+#include "xla/pjrt/c/pjrt_c_api_wrapper_impl.h"
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/pjrt/plugin/mlx/client_cpp_pjrt.h"
+
+namespace mlir::stablehlo {
+
+// Create my C++ client. Called by the C API and is the glue between the C API
+// and the C++ API.
+PJRT_Error* PJRT_StablehloMlxClient_Create(PJRT_Client_Create_Args* args) {
+  std::unique_ptr<xla::PjRtClient> client = CreateStablehloMlxPjrtClient();
+  args->client = pjrt::CreateWrapperClient(std::move(client));
+  // printf("Creating PJRT Client from client\n");
+  return nullptr;
+}
+
+PJRT_Error* PJRT_StablehloMlxExecuteContext_Create(
+    PJRT_ExecuteContext_Create_Args* args) {
+  return new PJRT_Error{absl::UnimplementedError(
+      "ExecuteContext not supported for StablehloMlx execution.")};
+}
+
+PJRT_Error* PJRT_StablehloMlxDeviceTopology_Create(
+    PJRT_TopologyDescription_Create_Args* args) {
+  return new PJRT_Error{absl::UnimplementedError(
+      "Topology not supported for StablehloMlx compilation.")};
+}
+
+}  // namespace mlir::stablehlo
+
+const PJRT_Api* GetPjrtApi() {
+  // printf("C++ Calling GetPjrtApi");
+  static PJRT_Layouts_Extension layouts_extension =
+      pjrt::CreateLayoutsExtension(nullptr);
+
+  static const PJRT_Api pjrt_api = pjrt::CreatePjrtApi(
+      mlir::stablehlo::PJRT_StablehloMlxClient_Create,
+      mlir::stablehlo::PJRT_StablehloMlxExecuteContext_Create,
+      mlir::stablehlo::PJRT_StablehloMlxDeviceTopology_Create,
+      pjrt::PJRT_Plugin_Initialize_NoOp,
+      reinterpret_cast<PJRT_Extension_Base*>(&layouts_extension),
+      pjrt::PJRT_Plugin_Attributes_Xla);
+
+  // printf("stablehlo_mlx client called GetPjrtApi\n");
+  return &pjrt_api;
+}
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/client_c_pjrt.h b/xla/pjrt/plugin/mlx/client_c_pjrt.h
new file mode 100644
index 0000000000..cc56626953
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/client_c_pjrt.h
@@ -0,0 +1,24 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_PJRT_PLUGIN_STABLEHLO_MLX_CLIENT_C_PJRT_H_
+#define XLA_PJRT_PLUGIN_STABLEHLO_MLX_CLIENT_C_PJRT_H_
+
+#include "xla/pjrt/c/pjrt_c_api.h"
+
+extern "C" {
+
+// Does not pass ownership of returned PJRT_Api* to caller.
+const PJRT_Api* GetPjrtApi();
+}
+
+#endif  // XLA_PJRT_PLUGIN_STABLEHLO_MLX_CLIENT_C_PJRT_H_
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/client_cpp_pjrt.cc b/xla/pjrt/plugin/mlx/client_cpp_pjrt.cc
new file mode 100644
index 0000000000..4c64c20b7b
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/client_cpp_pjrt.cc
@@ -0,0 +1,299 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/pjrt/plugin/mlx/client_cpp_pjrt.h"
+
+#include <cstddef>
+#include <cstdint>
+#include <cstdlib>
+#include <memory>
+#include <optional>
+#include <vector>
+
+#include "absl/status/statusor.h"
+#include "absl/strings/str_format.h"
+#include "absl/strings/string_view.h"
+#include "absl/types/span.h"
+#include "mlx/mlx.h"
+#include "xla/literal.h"
+#include "xla/pjrt/host_memory_spaces.h"
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/pjrt/pjrt_common.h"
+#include "xla/pjrt/pjrt_compiler.h"
+#include "xla/pjrt/plugin/mlx/buffer.h"
+#include "xla/pjrt/plugin/mlx/device.h"
+#include "xla/pjrt/plugin/mlx/executable.h"
+#include "xla/pjrt/plugin/mlx/logging.h"
+#include "xla/pjrt/plugin/mlx/utils.h"
+#include "xla/shape_util.h"
+#include "xla/util.h"
+#include "tsl/platform/fingerprint.h"
+#include "tsl/platform/statusor.h"
+
+namespace mx = mlx::core;
+
+#define UNIMPLEMENTED(name) \
+  xla::Unimplemented("MlxPjrtBuffer::" #name " is not implemented")
+
+namespace mlir::stablehlo {
+
+const xla::PjRtPlatformId kStablehloMlxBackendId =
+    tsl::Fingerprint64(kStablehloMlxBackendName);
+
+class StablehloMlxPjrtClient : public xla::PjRtClient {
+ public:
+  StablehloMlxPjrtClient()
+      : xla::PjRtClient(),
+        owned_devices_(),
+        devices_(),
+        owned_memory_space_(),
+        memory_space_(nullptr) {
+    // Init device and memory space.
+    // TRACE_ME_MEMBER;
+    owned_devices_.push_back(GetStablehloMlxDevice(this));
+    devices_.push_back(owned_devices_.back().get());
+    owned_memory_space_ = std::make_unique<xla::UnpinnedHostMemorySpace>(
+        /*id=*/0, devices_.front());
+    memory_space_ = owned_memory_space_.get();
+    AttachStablehloMlxMemorySpace(devices_.front(), memory_space_);
+    // mx::disable_compile();
+  }
+
+  ~StablehloMlxPjrtClient() override {
+    // mx::metal::stop_capture();
+  };
+
+  absl::string_view platform_name() const override {
+    // TRACE_ME_MEMBER;
+    return kStablehloMlxBackendName;
+  }
+  int process_index() const override {
+    // TRACE_ME_MEMBER;
+    return 0;
+  }
+
+  int device_count() const override {
+    // TRACE_ME_MEMBER;
+    return devices_.size();
+  }
+
+  int addressable_device_count() const override {
+    // TRACE_ME_MEMBER;
+    return devices_.size();
+  }
+
+  absl::Span<xla::PjRtDevice* const> devices() const override {
+    // TRACE_ME_MEMBER;
+    return devices_;
+  }
+  absl::Span<xla::PjRtDevice* const> addressable_devices() const override {
+    // TRACE_ME_MEMBER;
+    return devices_;
+  }
+
+  absl::Span<xla::PjRtMemorySpace* const> memory_spaces() const override {
+    // TRACE_ME_MEMBER;
+    return absl::MakeSpan(&memory_space_, 1);
+  }
+
+  // Return an ID that identifies the platform via tsl fingerprint.
+  xla::PjRtPlatformId platform_id() const override {
+    // TRACE_ME_MEMBER;
+    return kStablehloMlxBackendId;
+  }
+
+  // Returns a string containing human-readable, platform-specific version
+  // info (e.g. the CUDA version on GPU or libtpu version on Cloud TPU).
+  absl::string_view platform_version() const override {
+    // TRACE_ME_MEMBER;
+    return "StableHLO MLX v0.1";
+  }
+
+  /////////////
+  // Device
+  /////////////
+
+  // Lookup any PjRtDevice for a given PjRtDevice::id().
+  // TODO: Should this be a base class? I.e. why doesn't the base client have
+  // a vector a device pointers?
+  absl::StatusOr<xla::PjRtDevice*> LookupDevice(
+      xla::PjRtGlobalDeviceId global_device_id) const override {
+    // TRACE_ME_MEMBER;
+    for (auto device : devices_) {
+      if (device->global_device_id() == global_device_id) {
+        return device;
+      }
+    }
+    // TODO: This error should be a base class method since its used in tests.
+    return xla::InvalidArgument("No matching device found for device_id %d",
+                                global_device_id.value());
+  }
+
+  absl::StatusOr<xla::PjRtDevice*> LookupAddressableDevice(
+      xla::PjRtLocalDeviceId local_device_id) const override {
+    // TRACE_ME_MEMBER;
+
+    for (auto* device : addressable_devices()) {
+      if (local_device_id == device->local_device_id()) {
+        return device;
+      }
+    }
+    return xla::InvalidArgument(
+        "No matching device found for local_device_id %d",
+        local_device_id.value());
+  }
+
+  absl::StatusOr<xla::DeviceAssignment> GetDefaultDeviceAssignment(
+      int num_replicas, int num_partitions) const override {
+    // TRACE_ME_MEMBER;
+    xla::DeviceAssignment assignment(num_replicas, num_partitions);
+    for (int64_t i = 0; i < num_replicas; ++i) {
+      for (int64_t j = 0; j < num_partitions; ++j) {
+        auto idx = (i + (j * num_replicas)) % devices_.size();
+        assignment(i, j) = devices_[idx]->global_device_id().value();
+      }
+    }
+    return assignment;
+  }
+
+  /////////////////
+  // Buffer methods
+  /////////////////
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> CreateErrorBuffer(
+      absl::Status error, const xla::Shape& shape, xla::PjRtDevice* device) {
+    // Prefer memory space implementation, device holding buffer is
+    // deprecated.
+    return CreateErrorBuffer(error, shape,
+                             device->default_memory_space().value_or(nullptr));
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> CreateErrorBuffer(
+      absl::Status error, const xla::Shape& shape,
+      xla::PjRtMemorySpace* memory) {
+    return UNIMPLEMENTED(CreateErrorBuffer);
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> CreateUninitializedBuffer(
+      const xla::Shape& shape, xla::PjRtDevice* device) {
+    // TRACE_ME_MEMBER;
+    return CreateMlirBufferUninitialized(
+        shape, device->default_memory_space().value_or(nullptr));
+  }
+
+  absl::StatusOr<std::unique_ptr<PjRtClient::AsyncHostToDeviceTransferManager>>
+  CreateBuffersForAsyncHostToDevice(absl::Span<const xla::Shape> shapes,
+                                    xla::PjRtDevice* device) {
+    // TRACE_ME_MEMBER;
+    return CreateBuffersForAsyncHostToDevice(
+        shapes, device->default_memory_space().value_or(nullptr));
+  }
+
+  absl::StatusOr<std::unique_ptr<PjRtClient::AsyncHostToDeviceTransferManager>>
+  CreateBuffersForAsyncHostToDevice(absl::Span<const xla::Shape> shapes,
+                                    xla::PjRtMemorySpace* memory_space) {
+    // TRACE_ME_MEMBER;
+    return UNIMPLEMENTED(CreateBuffersForAsyncHostToDevice);
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> BufferFromHostBuffer(
+      const void* data, xla::PrimitiveType type, absl::Span<int64_t const> dims,
+      std::optional<absl::Span<int64_t const>> byte_strides,
+      HostBufferSemantics host_buffer_semantics,
+      absl::AnyInvocable<void() &&> on_done_with_host_buffer,
+      xla::PjRtDevice* device) {
+    // TRACE_ME_MEMBER;
+    return BufferFromHostBuffer(
+        data, type, dims, byte_strides, host_buffer_semantics,
+        std::move(on_done_with_host_buffer),
+        device->default_memory_space().value_or(nullptr), nullptr);
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> BufferFromHostBuffer(
+      const void* data, xla::PrimitiveType type, absl::Span<int64_t const> dims,
+      std::optional<absl::Span<int64_t const>> byte_strides,
+      HostBufferSemantics host_buffer_semantics,
+      absl::AnyInvocable<void() &&> on_done_with_host_buffer,
+      xla::PjRtDevice* device, const xla::Layout* device_layout) {
+    // TRACE_ME_MEMBER;
+    return BufferFromHostBuffer(
+        data, type, dims, byte_strides, host_buffer_semantics,
+        std::move(on_done_with_host_buffer),
+        device->default_memory_space().value_or(nullptr), device_layout);
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> BufferFromHostBuffer(
+      const void* data, xla::PrimitiveType type, absl::Span<int64_t const> dims,
+      std::optional<absl::Span<int64_t const>> byte_strides,
+      HostBufferSemantics host_buffer_semantics,
+      absl::AnyInvocable<void() &&> on_done_with_host_buffer,
+      xla::PjRtMemorySpace* memory_space, const xla::Layout* device_layout) {
+    // TRACE_ME_MEMBER;
+    TF_ASSIGN_OR_RETURN(
+        mx::array array_buffer,
+        utils::array::fromHostBuffer(data, dims, byte_strides, type));
+    auto buffer = CreateMlirBufferFromMlxArray(array_buffer, memory_space);
+    if (on_done_with_host_buffer) {
+      // If host is awaiting the result, must call this function.
+      std::move(on_done_with_host_buffer)();
+      on_done_with_host_buffer = nullptr;
+    }
+    return buffer;
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> BufferFromHostLiteral(
+      const xla::LiteralSlice& literal, xla::PjRtDevice* device) {
+    // TRACE_ME_MEMBER;
+    return CreateMlirBufferFromLiteral(
+        literal, device->default_memory_space().value_or(nullptr));
+  }
+
+  absl::StatusOr<std::unique_ptr<xla::PjRtBuffer>> BufferFromHostLiteral(
+      const xla::LiteralSlice& literal, xla::PjRtMemorySpace* memory_space) {
+    // TRACE_ME_MEMBER;
+    return CreateMlirBufferFromLiteral(literal, memory_space);
+  }
+
+  ///////////
+  // Compile
+  ///////////
+  absl::StatusOr<std::unique_ptr<xla::PjRtLoadedExecutable>> CompileAndLoad(
+      mlir::ModuleOp module, xla::CompileOptions options) override {
+    // TRACE_ME_MEMBER;
+    return mlir::stablehlo::StablehloMlxCompile(
+        module, GetDefaultDeviceAssignment(1, devices_.size()).value(), this);
+  }
+
+  // Compile `computation` with given `options`.
+  absl::StatusOr<std::unique_ptr<xla::PjRtLoadedExecutable>> CompileAndLoad(
+      const xla::XlaComputation& computation,
+      xla::CompileOptions options) override {
+    // TRACE_ME_MEMBER;
+    return mlir::stablehlo::StablehloMlxCompile(
+        computation.proto(),
+        GetDefaultDeviceAssignment(1, devices_.size()).value(), this);
+  }
+
+ private:
+  std::vector<std::unique_ptr<xla::PjRtDevice>> owned_devices_;
+  std::vector<xla::PjRtDevice*> devices_;
+  std::unique_ptr<xla::PjRtMemorySpace> owned_memory_space_;
+  xla::PjRtMemorySpace* memory_space_;
+};  // end class
+
+std::unique_ptr<xla::PjRtClient> CreateStablehloMlxPjrtClient() {
+  // mx::metal::start_capture("mlx_trace.gputrace");
+  SetupLogLevelFromEnv();
+  return std::make_unique<StablehloMlxPjrtClient>();
+}
+
+}  // namespace mlir::stablehlo
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/client_cpp_pjrt.h b/xla/pjrt/plugin/mlx/client_cpp_pjrt.h
new file mode 100644
index 0000000000..8c90942f0c
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/client_cpp_pjrt.h
@@ -0,0 +1,30 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_PJRT_PLUGIN_STABLEHLO_MLX_CLIENT_CPP_PJRT_H_
+#define XLA_PJRT_PLUGIN_STABLEHLO_MLX_CLIENT_CPP_PJRT_H_
+
+#include <memory>
+
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/pjrt/pjrt_compiler.h"
+
+namespace mlir::stablehlo {
+
+inline constexpr char kStablehloMlxBackendName[] = "stablehlo_mlx";
+extern const xla::PjRtPlatformId kStablehloMlxBackendId;
+
+std::unique_ptr<xla::PjRtClient> CreateStablehloMlxPjrtClient();
+
+}  // namespace mlir::stablehlo
+
+#endif  // XLA_PJRT_PLUGIN_STABLEHLO_MLX_CLIENT_CPP_PJRT_H_
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/compile.cc b/xla/pjrt/plugin/mlx/compile.cc
new file mode 100644
index 0000000000..70d9a620e1
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/compile.cc
@@ -0,0 +1,1305 @@
+#include "xla/pjrt/plugin/mlx/compile.h"
+
+#include <iostream>
+#include <algorithm>
+#include <memory>
+#include <optional>
+#include <utility>
+#include <cstddef>
+#include <functional>
+#include <unordered_map>
+#include <vector>
+#include <numeric>
+#include <string>
+#include <tuple>
+#include <type_traits>
+
+// TODO(@cryptodeal): might need to update `BUILD`
+#include "mlir/IR/Visitors.h"
+
+#include "mlx/mlx.h"
+#include "mlx/compile_impl.h"
+#include "absl/log/log.h"
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "mlir/Bytecode/BytecodeWriter.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/AsmState.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/OwningOpRef.h"
+#include "mlir/Interfaces/DataLayoutInterfaces.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "stablehlo/reference/Api.h"
+#include "stablehlo/transforms/optimization/Passes.h"
+#include "xla/hlo/translate/stablehlo.h"
+#include "xla/mlir/utils/error_util.h"
+#include "xla/mlir/utils/type_util.h"
+#include "xla/mlir_hlo/mhlo/transforms/passes.h"
+#include "xla/mlir_hlo/stablehlo_ext/transforms/passes.h"
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/pjrt/pjrt_executable.h"
+#include "xla/pjrt/pjrt_future.h"
+#include "xla/pjrt/plugin/mlx/buffer.h"
+#include "xla/pjrt/plugin/mlx/logging.h"
+#include "xla/pjrt/plugin/mlx/utils.h"
+#include "xla/service/computation_placer.h"
+#include "tsl/platform/status.h"
+#include "tsl/platform/statusor.h"
+
+namespace mx = mlx::core;
+
+namespace compile {
+
+namespace detail {
+// cbrt implementation
+std::vector<mx::array> cbrt(const std::vector<mx::array>& inputs) {
+  return std::vector<mx::array>{mx::power(
+      inputs[0], mx::full<float>(inputs[0].shape(), 1 / 3, inputs[0].dtype()))};
+}
+
+std::vector<mx::array> func(mlir::ModuleOp mod, mlir::func::FuncOp op,
+                            const std::vector<mx::array>& inputs,
+                            bool recursive_compile) {
+  auto& block = op.front();
+  std::unordered_map<mlir::Operation*, std::vector<mx::array>> block_ctx;
+  mlir::Operation* last_op = nullptr;
+  for (mlir::Operation& op : block.getOperations()) {
+    std::vector<mx::array> res =
+        llvm::TypeSwitch<mlir::Operation*, std::vector<mx::array>>(&op)
+            .Case<mlir::func::ReturnOp, mlir::stablehlo::ReturnOp>(
+                [&block_ctx, &inputs](auto o) {
+                  // std::cout << "return op:" << std::endl;
+                  std::vector<mx::array> res;
+                  for (mlir::Value val : o.getOperands()) {
+                    res.emplace_back(utils::array::fromOperand(val, block_ctx,
+                                                               inputs, false));
+                  }
+                  return res;
+                })
+            .Case<mlir::func::CallOp>(
+                [&block_ctx, &inputs, &mod, recursive_compile](auto o) {
+                  std::vector<mx::array> operands;
+                  for (mlir::Value val : o.getOperands()) {
+                    operands.emplace_back(utils::array::fromOperand(
+                        val, block_ctx, inputs, false));
+                  }
+                  auto func_op =
+                      mod.lookupSymbol<mlir::func::FuncOp>(o.getCallee());
+                  // std::cout << "Calling function: " <<
+                  // func_op.getName().str()
+                  //           << std::endl;
+                  if (recursive_compile) {
+                    return compile::func(mod, func_op)(operands);
+                  } else {
+                    return func(mod, func_op, operands, false);
+                  }
+                })
+            // Handle StableHLO nullary ops
+            .Case<mlir::stablehlo::ConstantOp>([](auto o) {
+              return std::vector<mx::array>{
+                  *utils::array::fromDenseElementsAttr(
+                      mlir::cast<mlir::DenseElementsAttr>(o.getValue()))};
+            })
+            .Case<mlir::stablehlo::IotaOp>([](auto o) {
+              auto result_type =
+                  mlir::cast<mlir::ShapedType>(o.getResult().getType());
+              mx::Shape result_shape(result_type.getShape().begin(),
+                                     result_type.getShape().end());
+              auto iota_dim = o.getIotaDimension();
+              mx::Dtype dtype =
+                  *utils::dtype::fromMlirType(result_type.getElementType());
+              mx::Shape broadcast_dims(result_shape.size(), 1);
+              broadcast_dims[iota_dim] = result_shape[iota_dim];
+              auto init_arange = mx::arange(
+                  static_cast<double>(result_shape[iota_dim]), dtype);
+              if (broadcast_dims.size()) {
+                init_arange = mx::reshape(init_arange, broadcast_dims);
+              }
+              return std::vector<mx::array>{
+                  mx::broadcast_to(init_arange, result_shape)};
+            })
+            // .Case<mlir::stablehlo::DynamicIotaOp>([](auto o) {})
+            /*
+              .Case<mlir::stablehlo::CreateTokenOp>([](auto o) {})
+              Deprecated see:
+                https://github.com/openxla/stablehlo/issues/2340
+                https://github.com/openxla/stablehlo/pull/2283
+            */
+            // Handle StableHLO unary elementwise op
+            .Case<mlir::stablehlo::AbsOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::abs(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::CbrtOp>([&block_ctx, &inputs](auto o) {
+              return mx::compile(cbrt)({utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs)});
+            })
+            .Case<mlir::stablehlo::CeilOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::ceil(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::ConvertOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::astype(
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                  *utils::dtype::fromMlirType(
+                      o.getResult().getType().getElementType()))};
+            })
+            // .Case<mlir::stablehlo::ClzOp>([](auto o) {})
+            .Case<mlir::stablehlo::CosineOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::cos(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::ExpOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::exp(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::Expm1Op>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::expm1(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::FloorOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::floor(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::ImagOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::imag(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::IsFiniteOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{
+                  mx::isfinite(utils::array::fromOperand(o.getOperand(),
+                                                         block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::LogOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::log(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::Log1pOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::log1p(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::LogisticOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{
+                  mx::sigmoid(utils::array::fromOperand(o.getOperand(),
+                                                        block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::NotOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{
+                  mx::logical_not(utils::array::fromOperand(
+                      o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::NegOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{
+                  mx::negative(utils::array::fromOperand(o.getOperand(),
+                                                         block_ctx, inputs))};
+            })
+            // .Case<mlir::stablehlo::PopulationCountOp>([](auto o) {})
+            .Case<mlir::stablehlo::RealOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::real(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            // TODO(@cryptodeal): `mlir::stablehlo::RoundOp` does not match with
+            // the mlx metal implementation
+            // .Case<mlir::stablehlo::RoundOp>([](auto o) {})
+            .Case<mlir::stablehlo::RoundNearestEvenOp>([&block_ctx,
+                                                        &inputs](auto o) {
+              return std::vector<mx::array>{mx::round(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::RsqrtOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::rsqrt(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::SignOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::sign(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::SineOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::sin(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::SqrtOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::sqrt(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::TanOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::tan(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::TanhOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::tanh(utils::array::fromOperand(
+                  o.getOperand(), block_ctx, inputs))};
+            })
+            // Handle StableHLO binary elementwise ops
+            .Case<mlir::stablehlo::AddOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::add(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::Atan2Op>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::arctan2(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            // TODO(@cryptodeal): implement complex op
+            // .Case<mlir::stablehlo::ComplexOp>([&block_ctx](auto
+            // op) {})
+            .Case<mlir::stablehlo::DivOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::divide(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::MaxOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::maximum(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::MinOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::minimum(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::MulOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::multiply(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::PowOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::power(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::RemOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::remainder(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::ShiftLeftOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::left_shift(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::ShiftRightArithmeticOp>(
+                [&block_ctx, &inputs](auto o) {
+                  /**
+                   * Per Metal Spec:
+                   * For the right-shift operator, if E1 has an unsigned
+                   * type or if E1 has a signed type and a nonnegative
+                   * value, the vacated bits are filled with zeros. If E1
+                   * has a signed type and a negative value, the vacated
+                   * bits are filled with ones.
+                   */
+                  auto lhs =
+                      utils::array::fromOperand(o.getLhs(), block_ctx, inputs);
+                  auto rhs =
+                      utils::array::fromOperand(o.getRhs(), block_ctx, inputs);
+                  auto target_dtype = rhs.dtype();
+                  switch (lhs.dtype().size()) {
+                    case 1:
+                      target_dtype = mx::int8;
+                      break;
+                    case 2:
+                      target_dtype = mx::int16;
+                      break;
+                    case 4:
+                      target_dtype = mx::int32;
+                      break;
+                    case 8:
+                      target_dtype = mx::int64;
+                      break;
+                  }
+                  return std::vector<mx::array>{
+                      mx::view(mx::right_shift(mx::view(lhs, target_dtype),
+                                               mx::astype(rhs, target_dtype)),
+                               lhs.dtype())};
+                })
+            .Case<mlir::stablehlo::ShiftRightLogicalOp>(
+                [&block_ctx, &inputs](auto o) {
+                  // Ensures that we bitcast to `uint` type before
+                  // performing the right shift. Should ensure that vacated
+                  // bits are zero populated.
+                  auto lhs =
+                      utils::array::fromOperand(o.getLhs(), block_ctx, inputs);
+                  auto rhs =
+                      utils::array::fromOperand(o.getRhs(), block_ctx, inputs);
+                  auto target_dtype = rhs.dtype();
+                  switch (lhs.dtype().size()) {
+                    case 1:
+                      target_dtype = mx::uint8;
+                      break;
+                    case 2:
+                      target_dtype = mx::uint16;
+                      break;
+                    case 4:
+                      target_dtype = mx::uint32;
+                      break;
+                    case 8:
+                      target_dtype = mx::uint64;
+                      break;
+                  }
+                  return std::vector<mx::array>{
+                      mx::view(mx::right_shift(mx::view(lhs, target_dtype),
+                                               mx::astype(rhs, target_dtype)),
+                               lhs.dtype())};
+                })
+            .Case<mlir::stablehlo::SubtractOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::subtract(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            // Handle StableHLO binary logical elementwise ops
+            .Case<mlir::stablehlo::AndOp>([&block_ctx, &inputs](auto o) {
+              auto lhs =
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs);
+              auto rhs =
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs);
+              switch (mx::kindof(lhs.dtype())) {
+                case mx::Dtype::Kind::b:
+                  return std::vector<mx::array>{mx::logical_and(lhs, rhs)};
+                default:
+                  return std::vector<mx::array>{mx::bitwise_and(lhs, rhs)};
+              }
+            })
+            .Case<mlir::stablehlo::OrOp>([&block_ctx, &inputs](auto o) {
+              auto lhs =
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs);
+              auto rhs =
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs);
+              switch (mx::kindof(lhs.dtype())) {
+                case mx::Dtype::Kind::b:
+                  return std::vector<mx::array>{mx::logical_or(lhs, rhs)};
+                default:
+                  return std::vector<mx::array>{mx::bitwise_or(lhs, rhs)};
+              }
+            })
+            .Case<mlir::stablehlo::XorOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::bitwise_xor(
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs))};
+            })
+            // Handle StableHLO communication ops
+            // .Case<mlir::stablehlo::InfeedOp>([](auto o) {})
+            // .Case<mlir::stablehlo::OutfeedOp>([](auto o) {})
+            // .Case<mlir::stablehlo::SendOp>([](auto o) {})
+            // .Case<mlir::stablehlo::RecvOp>([](auto o) {})
+
+            // Handle StableHLO parallelism related ops
+            // .Case<mlir::stablehlo::ReplicaIdOp>([](auto o) {})
+            // .Case<mlir::stablehlo::PartitionIdOp>([](auto o) {})
+
+            // Handle StableHLO control flow ops
+            // .Case<mlir::stablehlo::AfterAllOp>([](auto o) {})
+            // TODO(@cryptodeal): need custom kernels in order to
+            // remove the need to call `mx::eval` for the following ops
+            // .Case<mlir::stablehlo::IfOp>([](auto o))
+            // .Case<mlir::stablehlo::CaseOp>([](auto o))
+            // .Case<mlir::stablehlo::WhileOp>([](auto o))
+            // .Case<mlir::stablehlo::AllGatherOp>([](auto o) {})
+            // .Case<mlir::stablehlo::AllReduceOp>([](auto o) {})
+            // .Case<mlir::stablehlo::ReduceScatterOp>([](auto o) {})
+            // .Case<mlir::stablehlo::AllToAllOp>([](auto o) {})
+            .Case<mlir::stablehlo::ReduceOp>([&block_ctx, &inputs](auto o) {
+              auto operand =
+                  utils::array::fromOperand(o.getOperand(0), block_ctx, inputs);
+              if (utils::isArgMaxReduce(o)) {
+                // std::cout << "ArgMaxReduce op: " << std::endl;
+                auto axis = static_cast<int>(o.getDimensions()[0]);
+                auto indices = mx::argmax(operand, axis);
+
+                mx::Dtype result_type = *utils::dtype::fromMlirType(
+                    mlir::cast<mlir::ShapedType>(o.getResults()[1].getType())
+                        .getElementType());
+                return std::vector<mx::array>{mx::take(operand, indices, axis),
+                                              mx::astype(indices, result_type)};
+              }
+              if (utils::isSumReduce(o)) {
+                // std::cout << "SumReduce op: " << std::endl;
+                return std::vector<mx::array>{
+                    mx::sum(operand, static_cast<int>(o.getDimensions()[0]))};
+              }
+              if (utils::isMaxReduce(o)) {
+                // std::cout << "MaxReduce op: " << std::endl;
+                return std::vector<mx::array>{
+                    mx::max(operand, static_cast<int>(o.getDimensions()[0]))};
+              }
+              if (utils::isMinReduce(o)) {
+                // std::cout << "MinReduce op: " << std::endl;
+                return std::vector<mx::array>{
+                    mx::min(operand, static_cast<int>(o.getDimensions()[0]))};
+              }
+              // TODO(@cryptodeal): further cleanup module validation
+              // done via call to `Compile` to check for valid reduce types.
+
+              // For now, we assume `mx::any` if none of the above
+
+              // if (isAnyReduce(o)) {
+              // std::cout << "Unhandled reduce op: " << std::endl;
+              return std::vector<mx::array>{
+                  mx::any(operand, static_cast<int>(o.getDimensions()[0]))};
+              //}
+            })
+            // Handle StableHLO tuple ops
+            // .Case<mlir::stablehlo::GetTupleElementOp>([](auto o) {})
+            // .Case<mlir::stablehlo::TupleOp>([](auto o) {})
+            .Case<mlir::stablehlo::CompareOp>([&block_ctx, &inputs](auto o) {
+              auto lhs =
+                  utils::array::fromOperand(o.getLhs(), block_ctx, inputs);
+              auto rhs =
+                  utils::array::fromOperand(o.getRhs(), block_ctx, inputs);
+              switch (o.getComparisonDirection()) {
+                case mlir::stablehlo::ComparisonDirection::NE:
+                  return std::vector<mx::array>{mx::not_equal(lhs, rhs)};
+                case mlir::stablehlo::ComparisonDirection::GE:
+                  return std::vector<mx::array>{mx::greater_equal(lhs, rhs)};
+                case mlir::stablehlo::ComparisonDirection::GT:
+                  return std::vector<mx::array>{mx::greater(lhs, rhs)};
+                case mlir::stablehlo::ComparisonDirection::LE:
+                  return std::vector<mx::array>{mx::less_equal(lhs, rhs)};
+                case mlir::stablehlo::ComparisonDirection::LT:
+                  return std::vector<mx::array>{mx::less(lhs, rhs)};
+                case mlir::stablehlo::ComparisonDirection::EQ:
+                  return std::vector<mx::array>{mx::equal(lhs, rhs)};
+              }
+            })
+            // Handle StableHLO Slice ops
+            .Case<mlir::stablehlo::SliceOp>([&block_ctx, &inputs](auto o) {
+              std::vector<int32_t> start_indices(o.getStartIndices().begin(),
+                                                 o.getStartIndices().end());
+              std::vector<int32_t> limit_indices(o.getLimitIndices().begin(),
+                                                 o.getLimitIndices().end());
+              std::vector<int32_t> strides(o.getStrides().begin(),
+                                           o.getStrides().end());
+              return std::vector<mx::array>{mx::slice(
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                  start_indices, limit_indices, strides)};
+            })
+            .Case<mlir::stablehlo::DynamicSliceOp>([&block_ctx,
+                                                    &inputs](auto o) {
+              auto operand =
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs);
+              std::vector<mx::array> indices;
+              for (auto val : o.getStartIndices()) {
+                indices.emplace_back(
+                    utils::array::fromOperand(val, block_ctx, inputs));
+              }
+              auto start_indices = mx::concatenate(indices);
+              std::vector<int> axes(operand.ndim());
+              std::iota(axes.begin(), axes.end(), 0);
+              std::vector<int32_t> slice_sizes(o.getSliceSizes().begin(),
+                                               o.getSliceSizes().end());
+              return std::vector<mx::array>{
+                  mx::slice(operand, start_indices, axes, slice_sizes)};
+            })
+            .Case<mlir::stablehlo::DynamicUpdateSliceOp>([&block_ctx,
+                                                          &inputs](auto o) {
+              auto operand =
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs);
+              auto update =
+                  utils::array::fromOperand(o.getUpdate(), block_ctx, inputs);
+              std::vector<mx::array> indices;
+              for (auto val : o.getStartIndices()) {
+                indices.emplace_back(
+                    utils::array::fromOperand(val, block_ctx, inputs));
+              }
+
+              auto start_indices = mx::concatenate(indices);
+              std::vector<int32_t> axes(operand.ndim());
+              std::iota(axes.begin(), axes.end(), 0);
+              return std::vector<mx::array>{
+                  mx::slice_update(operand, update, start_indices, axes)};
+            })
+            // Handle StableHLO Other ops
+            // .Case<mlir::stablehlo::BatchNormGradOp>([](auto o) {})
+            // .Case<mlir::stablehlo::BatchNormInferenceOp>([](auto o) {})
+            // .Case<mlir::stablehlo::BatchNormTrainingOp>([](auto o) {})
+            .Case<mlir::stablehlo::BitcastConvertOp>([&block_ctx,
+                                                      &inputs](auto o) {
+              auto result_type = *utils::dtype::fromMlirType(
+                  mlir::cast<mlir::ShapedType>(o.getResult().getType())
+                      .getElementType());
+              return std::vector<mx::array>{mx::view(
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                  result_type)};
+            })
+            /*
+              .Case<mlir::stablehlo::BroadcastOp>([](auto o) {})
+              Deprecated see:
+                https://github.com/openxla/stablehlo/issues/2340
+                https://github.com/openxla/stablehlo/pull/2283
+            */
+            .Case<mlir::stablehlo::BroadcastInDimOp>([&block_ctx,
+                                                      &inputs](auto o) {
+              auto result_type =
+                  mlir::cast<mlir::ShapedType>(o.getResult().getType());
+              mx::Shape result_shape(result_type.getShape().begin(),
+                                     result_type.getShape().end());
+              std::vector<int32_t> broadcast_dims(
+                  o.getBroadcastDimensions().begin(),
+                  o.getBroadcastDimensions().end());
+              auto operand =
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs);
+
+              // potentially reshape to pad with ones, which allows
+              // broadcasting
+              auto min_rank = std::min(operand.ndim(), result_shape.size());
+              if (min_rank == operand.ndim()) {
+                mx::Shape padded_shape = operand.shape();
+                for (auto i = 0; i < result_shape.size(); i++) {
+                  if (auto search = std::find(broadcast_dims.begin(),
+                                              broadcast_dims.end(), i);
+                      search == broadcast_dims.end()) {
+                    padded_shape.insert(padded_shape.begin() + i, 1);
+                  }
+                }
+                operand = mx::reshape(operand, padded_shape);
+              }
+              return std::vector<mx::array>{
+                  mx::broadcast_to(operand, result_shape)};
+            })
+            // .Case<mlir::stablehlo::DynamicBroadcastInDimOp>([](auto o) {})
+            .Case<mlir::stablehlo::CholeskyOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::linalg::cholesky(
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                  o.getLower())};
+            })
+            .Case<mlir::stablehlo::ClampOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::clip(
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getMin(), block_ctx, inputs),
+                  utils::array::fromOperand(o.getMax(), block_ctx, inputs))};
+            })
+            .Case<mlir::stablehlo::ConcatenateOp>(
+                [&block_ctx, &inputs](auto o) {
+                  std::vector<mx::array> inputs;
+                  for (auto val : o.getOperands()) {
+                    inputs.emplace_back(
+                        utils::array::fromOperand(val, block_ctx, inputs));
+                  }
+                  return std::vector<mx::array>{
+                      mx::concatenate(inputs, o.getDimension())};
+                })
+            // .Case<mlir::stablehlo::CollectiveBroadcastOp>([](auto o) {})
+            // .Case<mlir::stablehlo::CollectivePermuteOp>([](auto o) {})
+            // .Case<mlir::stablehlo::CompositeOp>([](auto o) {})
+            // .Case<mlir::stablehlo::ConvolutionOp>([](auto o) {})
+            // .Case<mlir::stablehlo::CrossReplicaSumOp>([](auto o) {})
+            // .Case<mlir::stablehlo::CustomCallOp>([](auto o) {})
+            /*
+              .Case<mlir::stablehlo::DotOp>([](auto o) {})
+              Deprecated see:
+                https://github.com/openxla/stablehlo/issues/2340
+                https://github.com/openxla/stablehlo/pull/2283
+            */
+            .Case<mlir::stablehlo::DotGeneralOp>([&block_ctx, &inputs](auto o) {
+              return compile::dotGeneral(o)(
+                  {utils::array::fromOperand(o.getLhs(), block_ctx, inputs),
+                   utils::array::fromOperand(o.getRhs(), block_ctx, inputs)});
+            })
+            /*
+          .Case<mlir::stablehlo::EinsumOp>([](auto op) {})
+          .Case<mlir::stablehlo::UnaryEinsumOp>([](auto op) {})
+          Deprecated see:
+            https://github.com/openxla/stablehlo/issues/2340
+            https://github.com/openxla/stablehlo/pull/2283
+        */
+            // .Case<mlir::stablehlo::FftOp>([](auto op) {})
+            // TODO(@cryptodeal): definitely room for optimization here
+            .Case<mlir::stablehlo::GatherOp>([&block_ctx, &inputs](auto o) {
+              return compile::gather(o)(
+                  {utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                   utils::array::fromOperand(o.getStartIndices(), block_ctx,
+                                             inputs)});
+            })
+            .Case<mlir::stablehlo::GetDimensionSizeOp>([&block_ctx,
+                                                        &inputs](auto o) {
+              return std::vector<mx::array>{mx::array(
+                  (utils::array::fromOperand(o.getOperand(), block_ctx, inputs))
+                      .shape(o.getDimension()))};
+            })
+            // .Case<mlir::stablehlo::MapOp>([](auto o) {})
+            .Case<mlir::stablehlo::ReshapeOp>([&block_ctx, &inputs](auto o) {
+              auto result_type =
+                  mlir::cast<mlir::ShapedType>(o.getResult().getType());
+              return std::vector<mx::array>{mx::reshape(
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                  std::vector<int32_t>(result_type.getShape().begin(),
+                                       result_type.getShape().end()))};
+            })
+            // .Case<mlir::stablehlo::DynamicReshapeOp>([](auto o) {})
+            .Case<mlir::stablehlo::ScatterOp>([&block_ctx, &inputs](auto o) {
+              std::vector<mx::array> operands;
+              for (auto val : o.getInputs()) {
+                operands.emplace_back(
+                    utils::array::fromOperand(val, block_ctx, inputs));
+              }
+              // Add updates to input list
+              for (auto val : o.getUpdates()) {
+                operands.emplace_back(
+                    utils::array::fromOperand(val, block_ctx, inputs));
+              }
+              // Add scatter indices to input list
+              operands.emplace_back(utils::array::fromOperand(
+                  o.getScatterIndices(), block_ctx, inputs));
+              return compile::scatter(o)(operands);
+            })
+            .Case<mlir::stablehlo::SelectOp>([&block_ctx, &inputs](auto o) {
+              return std::vector<mx::array>{mx::where(
+                  utils::array::fromOperand(o.getOperand(0), block_ctx, inputs),
+                  utils::array::fromOperand(o.getOperand(1), block_ctx, inputs),
+                  utils::array::fromOperand(o.getOperand(2), block_ctx,
+                                            inputs))};
+            })
+            // .Case<mlir::stablehlo::SelectAndScatterOp>([](auto o) {})
+            // .Case<mlir::stablehlo::SetDimensionSizeOp>([](auto o) {})
+            .Case<mlir::stablehlo::SortOp>([&block_ctx, &inputs](auto o) {
+              std::vector<mx::array> tmp_inputs;
+              for (auto val : o.getInputs()) {
+                tmp_inputs.emplace_back(
+                    utils::array::fromOperand(val, block_ctx, inputs));
+              }
+              int dim = static_cast<int>(o.getDimension());
+              // Get update computation
+              mlir::Block& comparator = o.getComparator().front();
+              auto [index, comparator_type] = *utils::getSortInfo(comparator);
+              auto indices =
+                  mx::argsort(tmp_inputs[index], dim, comparator_type);
+              std::vector<mx::array> res;
+              mx::Shape slice_sizes(tmp_inputs[0].ndim(), 1);
+              for (auto in : tmp_inputs) {
+                res.emplace_back(mx::reshape(
+                    mx::gather(in, indices, dim, slice_sizes), in.shape()));
+              }
+              return res;
+            })
+            // .Case<mlir::stablehlo::ReverseOp>([](auto o) {})
+            // TODO (@cryptodeal): lots of room for optimization here
+            .Case<mlir::stablehlo::PadOp>([&block_ctx, &inputs](auto o) {
+              return compile::pad(o)(
+                  {utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                   utils::array::fromOperand(o.getPaddingValue(), block_ctx,
+                                             inputs)});
+            })
+            .Case<mlir::stablehlo::TransposeOp>([&block_ctx, &inputs](auto o) {
+              std::vector<int> axes(o.getPermutation().begin(),
+                                    o.getPermutation().end());
+              return std::vector<mx::array>{mx::transpose(
+                  utils::array::fromOperand(o.getOperand(), block_ctx, inputs),
+                  axes)};
+            })
+            // .Case<mlir::stablehlo::TriangularSolveOp>([](auto o) {})
+            // .Case<mlir::stablehlo::ReduceWindowOp>([](auto o) {})
+            // .Case<mlir::stablehlo::ReturnOp>([](auto o) {})
+            // .Case<mlir::stablehlo::TorchIndexSelectOp>([](auto o) {})
+            // .Case<mlir::stablehlo::OptimizationBarrierOp>([](auto o) {})
+            // .Case<mlir::stablehlo::CrossReplicaSumOp>([](auto o) {})
+            // Need to modify such that `rng.state` is held outside of scope
+            // of `mx::compile`
+            // TODO(@cryptodeal): re-enable once we've handled ensuring
+            // state is handled correctly
+            // .Case<mlir::stablehlo::RngOp>([&block_ctx](auto o) {
+            //   auto rng_func = [&o](const std::vector<mx::array>& inputs)
+            //   {
+            //     auto a = inputs[0];
+            //     auto b = inputs[1];
+            //     auto result_type =
+            //         mlir::cast<mlir::ShapedType>(o.getResult().getType());
+            //     std::vector<int32_t>
+            //     shape(result_type.getShape().begin(),
+            //                                result_type.getShape().end());
+
+            //     switch (o.getRngDistribution()) {
+            //       case RngDistribution::UNIFORM:
+            //         return std::vector<mx::array>{
+            //             mx::random::uniform(a, b, shape, a.dtype())};
+            //       default:
+            //         return std::vector<mx::array>{
+            //             mx::random::normal(shape, a.dtype(), a, b)};
+            //     }
+            //   };
+            //   return mx::compile(rng_func)(
+            //       {utils::array::fromOperand(o.getA(), block_ctx),
+            //        utils::array::fromOperand(o.getB(), block_ctx)});
+            // })
+            // .Case<mlir::stablehlo::RngBitGeneratorOp>([&block_ctx](auto o) {
+            //   auto rng_bit_gen_func = [&o](const std::vector<mx::array>&
+            //   inputs) {
+
+            //   };
+            //   return mx::compile(rng_bit_gen_func)(
+            //       {utils::array::fromOperand(o.getA(), block_ctx),
+            //        utils::array::fromOperand(o.getB(), block_ctx)});
+            // })
+            // Handle StableHLO Quantize ops
+            // .Case<mlir::stablehlo::UniformQuantizeOp>([](auto op) {})
+            // .Case<mlir::stablehlo::UniformDequantizeOp>([](auto op) {})
+            // .Case<mlir::stablehlo::ReducePrecisionOp>([](auto op) {})
+            /*
+              .Case<mlir::stablehlo::RealDynamicSliceOp>([](auto o) {})
+              Deprecated see:
+                https://github.com/openxla/stablehlo/issues/2340
+                https://github.com/openxla/stablehlo/pull/2283
+            */
+            // .Case<mlir::stablehlo::DynamicPadOp>([](auto o) {})
+            // .Case<mlir::stablehlo::DynamicGatherOp>([](auto o) {})
+            // .Case<mlir::stablehlo::DynamicConvOp>([](auto o) {})
+
+            .Default([&op](auto o) {
+              // Shouldn't be possible to reach this point, return
+              // empty vector.
+              return std::vector<mx::array>{};
+            });
+    block_ctx.emplace(&op, std::move(res));
+    last_op = &op;
+  }
+  return block_ctx.find(last_op)->second;
+}
+}  // namespace detail
+
+MlxFunc dotGeneral(mlir::stablehlo::DotGeneralOp op) {
+  auto dot_dim_nums = op.getDotDimensionNumbers();
+  std::vector<int32_t> lhs_batch_dims(
+      dot_dim_nums.getLhsBatchingDimensions().begin(),
+      dot_dim_nums.getLhsBatchingDimensions().end());
+  std::vector<int32_t> rhs_batch_dims(
+      dot_dim_nums.getRhsBatchingDimensions().begin(),
+      dot_dim_nums.getRhsBatchingDimensions().end());
+  std::vector<int32_t> lhs_contract_dims(
+      dot_dim_nums.getLhsContractingDimensions().begin(),
+      dot_dim_nums.getLhsContractingDimensions().end());
+  std::vector<int32_t> rhs_contract_dims(
+      dot_dim_nums.getRhsContractingDimensions().begin(),
+      dot_dim_nums.getRhsContractingDimensions().end());
+
+  MlxFunc fun = [lhs_batch_dims = std::move(lhs_batch_dims),
+                 rhs_batch_dims = std::move(rhs_batch_dims),
+                 lhs_contract_dims = std::move(lhs_contract_dims),
+                 rhs_contract_dims = std::move(rhs_contract_dims)](
+                    const std::vector<mx::array>& inputs) {
+    auto lhs = inputs[0];
+    auto rhs = inputs[1];
+
+    int dim_count = 0;
+    auto getDimChar = [&dim_count]() -> char { return 'a' + dim_count++; };
+
+    std::unordered_map<int, char> lhs_batch_map;
+    std::unordered_map<int, char> rhs_batch_map;
+    std::unordered_map<int, char> lhs_contract_map;
+    std::unordered_map<int, char> rhs_contract_map;
+
+    std::string res_subscript;
+    for (auto i = 0; i < lhs_batch_dims.size(); ++i) {
+      auto dim_char = getDimChar();
+      res_subscript = res_subscript + dim_char;
+      lhs_batch_map.emplace(lhs_batch_dims[i], dim_char);
+      rhs_batch_map.emplace(rhs_batch_dims[i], dim_char);
+    }
+
+    for (auto i = 0; i < lhs_contract_dims.size(); ++i) {
+      auto dim_char = getDimChar();
+      lhs_contract_map.emplace(lhs_contract_dims[i], dim_char);
+      rhs_contract_map.emplace(rhs_contract_dims[i], dim_char);
+    }
+
+    std::string lhs_subscript;
+    for (auto i = 0; i < lhs.ndim(); ++i) {
+      if (auto match = lhs_batch_map.find(i); match != lhs_batch_map.end()) {
+        lhs_subscript = lhs_subscript + match->second;
+      } else if (auto match = lhs_contract_map.find(i);
+                 match != lhs_contract_map.end()) {
+        lhs_subscript = lhs_subscript + match->second;
+      } else {
+        auto dim_char = getDimChar();
+        res_subscript = res_subscript + dim_char;
+        lhs_subscript = lhs_subscript + dim_char;
+      }
+    }
+
+    std::string rhs_subscript;
+    for (auto i = 0; i < rhs.ndim(); ++i) {
+      if (auto match = rhs_batch_map.find(i); match != rhs_batch_map.end()) {
+        rhs_subscript = rhs_subscript + match->second;
+      } else if (auto match = rhs_contract_map.find(i);
+                 match != rhs_contract_map.end()) {
+        rhs_subscript = rhs_subscript + match->second;
+      } else {
+        auto dim_char = getDimChar();
+        res_subscript = res_subscript + dim_char;
+        rhs_subscript = rhs_subscript + dim_char;
+      }
+    }
+
+    return std::vector<mx::array>{mx::einsum(
+        lhs_subscript + "," + rhs_subscript + "->" + res_subscript, inputs)};
+  };
+  return mx::compile(std::move(fun));
+}
+
+MlxFunc gather(mlir::stablehlo::GatherOp op) {
+  auto dim_nums = op.getDimensionNumbers();
+  std::vector<int32_t> offset_dims(dim_nums.getOffsetDims().begin(),
+                                   dim_nums.getOffsetDims().end());
+  std::vector<int32_t> collapsed_slice_dims(
+      dim_nums.getCollapsedSliceDims().begin(),
+      dim_nums.getCollapsedSliceDims().end());
+  std::vector<int32_t> operand_batching_dims(
+      dim_nums.getOperandBatchingDims().begin(),
+      dim_nums.getOperandBatchingDims().end());
+  std::vector<int32_t> start_indices_batching_dims(
+      dim_nums.getStartIndicesBatchingDims().begin(),
+      dim_nums.getStartIndicesBatchingDims().end());
+  std::vector<int32_t> start_index_map(dim_nums.getStartIndexMap().begin(),
+                                       dim_nums.getStartIndexMap().end());
+  auto index_vector_dim = dim_nums.getIndexVectorDim();
+  std::vector<int32_t> slice_sizes(op.getSliceSizes().begin(),
+                                   op.getSliceSizes().end());
+  MlxFunc fun = [collapsed_slice_dims = std::move(collapsed_slice_dims),
+                 index_vector_dim, offset_dims = std::move(offset_dims),
+                 operand_batching_dims = std::move(operand_batching_dims),
+                 slice_sizes = std::move(slice_sizes),
+                 start_index_map = std::move(start_index_map),
+                 start_indices_batching_dims =
+                     std::move(start_indices_batching_dims)](
+                    const std::vector<mx::array>& inputs) {
+    auto operand = inputs[0];
+    auto start_indices = inputs[1];
+
+    std::vector<int32_t> batch_dim_sizes = start_indices.shape();
+    if (index_vector_dim < batch_dim_sizes.size()) {
+      batch_dim_sizes.erase(batch_dim_sizes.begin() + index_vector_dim);
+    }
+
+    std::vector<int32_t> offset_dim_sizes;
+    for (auto i = 0; i < slice_sizes.size(); ++i) {
+      if (std::find(collapsed_slice_dims.begin(), collapsed_slice_dims.end(),
+                    i) != collapsed_slice_dims.end() ||
+          std::find(operand_batching_dims.begin(), operand_batching_dims.end(),
+                    i) != operand_batching_dims.end()) {
+        continue;
+      }
+      offset_dim_sizes.push_back(slice_sizes[i]);
+    }
+
+    // Calculate result shape and batch dims
+    std::vector<int32_t> result_shape(batch_dim_sizes.size() +
+                                      offset_dim_sizes.size());
+    std::vector<int32_t> batch_dims_vec(batch_dim_sizes.size());
+    auto batch_dims_count = 0;
+    auto offset_dims_count = 0;
+    for (auto i = 0; i < result_shape.size(); ++i) {
+      if (auto search = std::find(offset_dims.begin(), offset_dims.end(), i);
+          search == offset_dims.end()) {
+        batch_dims_vec[batch_dims_count] = i;
+        result_shape[i] = batch_dim_sizes[batch_dims_count++];
+      } else {
+        result_shape[i] = offset_dim_sizes[offset_dims_count++];
+      }
+    }
+
+    mx::array batch_dims = mx::array(
+        batch_dims_vec.data(), {static_cast<int32_t>(batch_dims_vec.size())});
+    mx::array start_idx_map = mx::array(
+        start_index_map.data(), {static_cast<int32_t>(start_index_map.size())});
+    mx::array op_batching_dims =
+        mx::array(operand_batching_dims.data(),
+                  {static_cast<int32_t>(operand_batching_dims.size())});
+
+    // Calculate result indices
+    std::vector<mx::array> meshgrid_arrays;
+    for (const auto& dim : result_shape) {
+      meshgrid_arrays.push_back(mx::arange(dim));
+    }
+    mx::array result_indices =
+        mx::reshape(mx::stack(mx::meshgrid(meshgrid_arrays, false, "ij"), -1),
+                    {-1, static_cast<int>(result_shape.size())});
+
+    // Initialize operand indices with zeros
+    mx::array operand_indices = mx::zeros(
+        {result_indices.shape(0), static_cast<int32_t>(operand.ndim())},
+        mx::int32);
+
+    // Slice batch indices
+    mx::array batch_indices = mx::take(result_indices, batch_dims, 1);
+
+    // Calculate start indices
+    std::vector<mx::array> tmp_indices;
+    if (batch_indices.size()) {
+      tmp_indices = mx::split(batch_indices, batch_indices.shape(1), 1);
+    }
+    for (auto& idx : tmp_indices) {
+      idx = mx::flatten(idx);
+    }
+
+    std::vector<int> tmp_axes;
+    for (auto i = 0; i < start_indices.ndim(); ++i) {
+      if (i == index_vector_dim) continue;
+      tmp_axes.push_back(i);
+    }
+    std::vector<int32_t> tmp_slice_sizes(start_indices.ndim(), 1);
+    if (index_vector_dim < tmp_slice_sizes.size()) {
+      tmp_slice_sizes[index_vector_dim] = start_indices.shape(index_vector_dim);
+    }
+
+    mx::array batch_start_indices =
+        mx::gather(start_indices, tmp_indices, tmp_axes, tmp_slice_sizes);
+    if (tmp_axes.size()) {
+      batch_start_indices = mx::reshape(
+          mx::gather(start_indices, tmp_indices, tmp_axes, tmp_slice_sizes),
+          {result_indices.shape(0), -1});
+    } else if (batch_start_indices.ndim() == 1) {
+      batch_start_indices = mx::broadcast_to(
+          mx::reshape(batch_start_indices, {1, batch_start_indices.shape(0)}),
+          {result_indices.shape(0), batch_start_indices.shape(0)});
+    }
+
+    // Seed with full start indices
+    std::vector<int32_t> max;
+    for (auto d_start = 0; d_start < start_index_map.size(); ++d_start) {
+      int d_operand = start_index_map[d_start];
+      max.push_back(operand.shape(d_operand) - slice_sizes[d_operand]);
+    }
+
+    operand_indices = mx::put_along_axis(
+        operand_indices,
+        mx::broadcast_to(start_idx_map,
+                         {operand_indices.shape(0),
+                          static_cast<int32_t>(start_index_map.size())}),
+        mx::clip(
+            mx::take(batch_start_indices,
+                     mx::arange(static_cast<int>(start_index_map.size())), 1),
+            mx::zeros(batch_start_indices.shape(), mx::int32),
+            mx::array(max.data(), {static_cast<int32_t>(max.size())})),
+        1);
+
+    // Calculate full batching indices
+    mx::array i_batching =
+        mx::arange(static_cast<int>(operand_batching_dims.size()));
+    mx::array d_operands = mx::take(op_batching_dims, i_batching);
+    mx::array d_starts = mx::take(
+        mx::array(start_indices_batching_dims.data(),
+                  {static_cast<int32_t>(start_indices_batching_dims.size())}),
+        i_batching);
+    auto tmp_batch_start_indices = mx::subtract(
+        d_starts, mx::where(mx::less(d_starts, mx::full(d_starts.shape(),
+                                                        index_vector_dim)),
+                            mx::zeros(d_starts.shape(), mx::int32),
+                            mx::ones(d_starts.shape(), mx::int32)));
+
+    // Add batching indices
+    operand_indices = mx::scatter_add_axis(
+        operand_indices,
+        mx::broadcast_to(d_operands,
+                         {operand_indices.shape(0), d_operands.shape(0)}),
+        mx::take(batch_indices, tmp_batch_start_indices, 1), 1);
+
+    // Calculate offset indices
+    mx::array offset_indices =
+        mx::take(result_indices,
+                 mx::array(offset_dims.data(),
+                           {static_cast<int32_t>(offset_dims.size())}),
+                 1);
+
+    std::vector<int32_t> result_offset_idx;
+    for (unsigned i = 0; i < operand_indices.shape(1); i++) {
+      if (std::find(operand_batching_dims.begin(), operand_batching_dims.end(),
+                    static_cast<int64_t>(i)) != operand_batching_dims.end() ||
+          std::find(collapsed_slice_dims.begin(), collapsed_slice_dims.end(),
+                    static_cast<int64_t>(i)) != collapsed_slice_dims.end()) {
+        continue;
+      }
+      result_offset_idx.push_back(i);
+    }
+
+    // Add offset indices
+    operand_indices = mx::scatter_add_axis(
+        operand_indices,
+        mx::broadcast_to(
+            mx::array(result_offset_idx.data(),
+                      {static_cast<int32_t>(result_offset_idx.size())}),
+            {operand_indices.shape(0),
+             static_cast<int32_t>(result_offset_idx.size())}),
+        offset_indices, 1);
+
+    std::vector<int32_t> gather_axes(operand.ndim());
+    std::iota(gather_axes.begin(), gather_axes.end(), 0);
+    return std::vector<mx::array>{mx::reshape(
+        mx::gather(operand,
+                   mx::split(operand_indices, operand_indices.shape(1), 1),
+                   gather_axes, std::vector<int32_t>(operand.ndim(), 1)),
+        result_shape)};
+  };
+  return mx::compile(fun);
+}
+
+MlxFunc pad(mlir::stablehlo::PadOp op) {
+  std::vector<int32_t> edge_pad_low(op.getEdgePaddingLow().begin(),
+                                    op.getEdgePaddingLow().end());
+  std::vector<int32_t> edge_pad_high(op.getEdgePaddingHigh().begin(),
+                                     op.getEdgePaddingHigh().end());
+  std::vector<int32_t> interior_pad(op.getInteriorPadding().begin(),
+                                    op.getInteriorPadding().end());
+
+  MlxFunc fun = [edge_pad_low = std::move(edge_pad_low),
+                 edge_pad_high = std::move(edge_pad_high),
+                 interior_pad = std::move(interior_pad)](
+                    const std::vector<mx::array>& inputs) {
+    auto operand = inputs[0];
+    auto padding_value = inputs[1];
+
+    std::vector<int32_t> result_shape(operand.ndim());
+    for (auto i = 0; i < result_shape.size(); ++i) {
+      result_shape[i] = operand.shape(i) + edge_pad_low[i] +
+                        std::max(operand.shape(i) - 1, 0) * interior_pad[i] +
+                        edge_pad_high[i];
+    }
+
+    auto edge_padding_low =
+        mx::array(edge_pad_low.data(),
+                  {static_cast<int32_t>(edge_pad_low.size())}, mx::int32);
+    auto interior_padding =
+        mx::array(interior_pad.data(),
+                  {static_cast<int32_t>(edge_pad_low.size())}, mx::int32);
+
+    // Calculate operand indices
+    std::vector<mx::array> meshgrid_arrays;
+    for (const auto& dim : operand.shape()) {
+      meshgrid_arrays.push_back(mx::arange(dim));
+    }
+    mx::array operand_indices =
+        mx::reshape(mx::stack(mx::meshgrid(meshgrid_arrays, false, "ij"), -1),
+                    {-1, static_cast<int>(operand.ndim())});
+
+    mx::array result_indices =
+        edge_padding_low +
+        operand_indices * (interior_padding + mx::array(1, mx::int32));
+
+    // initialize array full of padding values
+    mx::array result = mx::full(result_shape, padding_value);
+    std::vector<int32_t> axes(operand.ndim());
+    std::iota(axes.begin(), axes.end(), 0);
+    return std::vector<mx::array>{mx::scatter(
+        result, mx::split(result_indices, result_indices.shape(1), 1),
+        mx::gather(operand,
+                   mx::split(operand_indices, operand_indices.shape(1), 1),
+                   axes, std::vector<int32_t>(operand.ndim(), 1)),
+        axes)};
+  };
+  return mx::compile(fun);
+}
+
+MlxFunc scatter(mlir::stablehlo::ScatterOp op) {
+  // Get scatter dimension numbers
+  auto scatter_dim_nums = op.getScatterDimensionNumbers();
+  auto index_vector_dim = scatter_dim_nums.getIndexVectorDim();
+  std::vector<int32_t> input_batching_dims(
+      scatter_dim_nums.getInputBatchingDims().begin(),
+      scatter_dim_nums.getInputBatchingDims().end());
+  std::vector<int32_t> inserted_window_dims(
+      scatter_dim_nums.getInsertedWindowDims().begin(),
+      scatter_dim_nums.getInsertedWindowDims().end());
+  std::vector<int32_t> scatter_dims_to_operand_dims(
+      scatter_dim_nums.getScatterDimsToOperandDims().begin(),
+      scatter_dim_nums.getScatterDimsToOperandDims().end());
+  std::vector<int32_t> scatter_indices_batching_dims(
+      scatter_dim_nums.getScatterIndicesBatchingDims().begin(),
+      scatter_dim_nums.getScatterIndicesBatchingDims().end());
+  std::vector<int32_t> update_window_dims(
+      scatter_dim_nums.getUpdateWindowDims().begin(),
+      scatter_dim_nums.getUpdateWindowDims().end());
+  mlir::Block& update_computation = op.getUpdateComputation().front();
+  // TODO(@cryptodeal): need to experiment to see how `computeHash`
+  // handles hashing `update_computation` property.
+  utils::ScatterType scatter_type = *utils::getScatterType(update_computation);
+
+  // mlir::Block& update_computation = o.getUpdateComputation().front();
+  // utils::ScatterType scatter_type = *getScatterType(update_computation);
+
+  MlxFunc fun = [index_vector_dim,
+                 input_batching_dims = std::move(input_batching_dims),
+                 inserted_window_dims = std::move(inserted_window_dims),
+                 scatter_dims_to_operand_dims =
+                     std::move(scatter_dims_to_operand_dims),
+                 scatter_indices_batching_dims =
+                     std::move(scatter_indices_batching_dims),
+                 scatter_type,
+                 update_window_dims = std::move(update_window_dims)](
+                    const std::vector<mx::array>& in) {
+    auto input_count = (in.size() - 1) / 2;
+    std::vector<mx::array> inputs(in.begin(), in.begin() + input_count);
+    std::vector<mx::array> updates(in.begin() + input_count, in.end() - 1);
+    auto scatter_indices = in[in.size() - 1];
+    // Calculate update scatter dims
+    std::vector<int32_t> update_scatter_dims;
+    for (auto i = 0; i < updates[0].ndim(); i++) {
+      if (std::find(update_window_dims.begin(), update_window_dims.end(),
+                    static_cast<int64_t>(i)) == update_window_dims.end()) {
+        update_scatter_dims.emplace_back(static_cast<int32_t>(i));
+      }
+    }
+
+    // Calculate update indices
+    std::vector<mx::array> meshgrid_arrays;
+    for (const auto& dim : updates[0].shape()) {
+      meshgrid_arrays.push_back(mx::arange(dim));
+    }
+    mx::array update_indices =
+        mx::reshape(mx::stack(mx::meshgrid(meshgrid_arrays, false, "ij"), -1),
+                    {-1, static_cast<int>(updates[0].ndim())});
+
+    // Calculate update scatter index
+    mx::array update_scatter_indices =
+        mx::take(update_indices,
+                 mx::array(update_scatter_dims.data(),
+                           {static_cast<int32_t>(update_scatter_dims.size())}),
+                 1);
+
+    // Initialize result indices with zeros
+    auto result_indices = mx::zeros(
+        {update_indices.shape(0), static_cast<int32_t>(inputs[0].ndim())},
+        mx::int32);
+
+    // Gather start indices; populating the corresponding
+    // result indices.
+    auto gather_dim_count = index_vector_dim < scatter_indices.ndim()
+                                ? update_scatter_dims.size() + 1
+                                : update_scatter_dims.size();
+    std::vector<int32_t> tmp_gather_axes(update_scatter_dims.size());
+    std::iota(tmp_gather_axes.begin(), tmp_gather_axes.end(), 0);
+    std::vector<int32_t> gather_slice_sizes(gather_dim_count, 1);
+    if (index_vector_dim < scatter_indices.ndim()) {
+      gather_slice_sizes[index_vector_dim] =
+          scatter_indices.shape(index_vector_dim);
+    }
+    mx::array start_indices =
+        mx::reshape(mx::gather(scatter_indices,
+                               mx::split(update_scatter_indices,
+                                         update_scatter_indices.shape(1), 1),
+                               tmp_gather_axes, gather_slice_sizes),
+                    {update_indices.shape(0), -1});
+
+    // Seed with full start indices
+    result_indices = mx::put_along_axis(
+        result_indices,
+        mx::broadcast_to(
+            mx::array(
+                scatter_dims_to_operand_dims.data(),
+                {static_cast<int32_t>(scatter_dims_to_operand_dims.size())}),
+            {result_indices.shape(0),
+             static_cast<int32_t>(scatter_dims_to_operand_dims.size())}),
+        start_indices, 1);
+
+    // Calculate full batching indices; accumulating to the
+    // corresponding result indices.
+    auto d_input =
+        mx::array(input_batching_dims.data(),
+                  {static_cast<int32_t>(input_batching_dims.size())});
+    auto d_start =
+        mx::array(scatter_indices_batching_dims.data(),
+                  {static_cast<int32_t>(scatter_indices_batching_dims.size())});
+    result_indices = mx::scatter_add_axis(
+        result_indices,
+        mx::broadcast_to(d_input, {result_indices.shape(0), d_input.shape(0)}),
+        mx::take(update_scatter_indices,
+                 mx::subtract(
+                     d_start,
+                     mx::where(mx::less(d_start, mx::full(d_start.shape(),
+                                                          index_vector_dim)),
+                               mx::zeros(d_start.shape(), mx::int32),
+                               mx::ones(d_start.shape(), mx::int32))),
+                 1),
+        1);
+
+    // Calculate update window indices
+    auto update_window_indices =
+        mx::take(update_indices,
+                 mx::array(update_window_dims.data(),
+                           {static_cast<int32_t>(update_window_dims.size())}),
+                 1);
+
+    // Calculate full window indices; accumulating to the
+    // corresponding result indices.
+    std::vector<int32_t> tmp_indices;
+    auto full_window_idx_count = static_cast<int32_t>(
+        update_window_indices.shape(1) + inserted_window_dims.size() +
+        input_batching_dims.size());
+    for (auto i = 0; i < full_window_idx_count; ++i) {
+      if (std::find(inserted_window_dims.begin(), inserted_window_dims.end(),
+                    i) != inserted_window_dims.end() ||
+          std::find(input_batching_dims.begin(), input_batching_dims.end(),
+                    i) != input_batching_dims.end()) {
+        continue;
+      }
+      tmp_indices.push_back(i);
+    }
+    result_indices = mx::scatter_add_axis(
+        result_indices,
+        mx::broadcast_to(mx::array(tmp_indices.data(),
+                                   {static_cast<int32_t>(tmp_indices.size())}),
+                         {result_indices.shape(0),
+                          static_cast<int32_t>(tmp_indices.size())}),
+        update_window_indices, 1);
+
+    std::vector<int32_t> scatter_axes(inputs[0].ndim());
+    std::iota(scatter_axes.begin(), scatter_axes.end(), 0);
+    std::vector<int32_t> gather_axes(updates[0].ndim());
+    std::iota(gather_axes.begin(), gather_axes.end(), 0);
+    std::vector<mx::array> res;
+
+    auto tmp_gather_indices =
+        mx::split(update_indices, update_indices.shape(1), 1);
+    auto tmp_scatter_indices =
+        mx::split(result_indices, result_indices.shape(1), 1);
+
+    std::vector<int32_t> update_shape(
+        inputs[0].ndim() + tmp_scatter_indices[0].ndim(), 1);
+    update_shape[0] = update_indices.shape(0);
+    std::vector<int32_t> tmp_gather_slice_sizes(updates[0].ndim(), 1);
+    for (auto i = 0; i < inputs.size(); ++i) {
+      auto update_vals =
+          mx::reshape(mx::gather(updates[i], tmp_gather_indices, gather_axes,
+                                 tmp_gather_slice_sizes),
+                      update_shape);
+      switch (scatter_type) {
+        case utils::ScatterType::Replace:
+          res.push_back(mx::scatter(inputs[i], tmp_scatter_indices, update_vals,
+                                    scatter_axes));
+          break;
+        case utils::ScatterType::Add:
+          res.push_back(mx::scatter_add(inputs[i], tmp_scatter_indices,
+                                        update_vals, scatter_axes));
+          break;
+        case utils::ScatterType::Prod:
+          res.push_back(mx::scatter_prod(inputs[i], tmp_scatter_indices,
+                                         update_vals, scatter_axes));
+          break;
+        case utils::ScatterType::Max:
+          res.push_back(mx::scatter_max(inputs[i], tmp_scatter_indices,
+                                        update_vals, scatter_axes));
+          break;
+        case utils::ScatterType::Min:
+          res.push_back(mx::scatter_min(inputs[i], tmp_scatter_indices,
+                                        update_vals, scatter_axes));
+          break;
+      }
+    }
+    return res;
+  };
+  return mx::compile(fun);
+}
+
+MlxFunc func(mlir::ModuleOp mod, mlir::func::FuncOp op) {
+  MlxFunc fun = [func_info = std::shared_ptr<utils::FuncInfo>(
+                     new utils::FuncInfo(mod, op.getName().str()))](
+                    const std::vector<mx::array>& inputs) mutable {
+    auto mod = func_info->mod_.get();
+    auto op = mod.lookupSymbol<mlir::func::FuncOp>(func_info->name_);
+    return detail::func(mod, op, inputs);
+  };
+  return mx::compile(std::move(fun));
+}
+
+}  // namespace compile
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/compile.h b/xla/pjrt/plugin/mlx/compile.h
new file mode 100644
index 0000000000..af5cc7e342
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/compile.h
@@ -0,0 +1,30 @@
+#ifndef XLA_PJRT_PLUGIN_STABLEHLO_MLX_COMPILE_H_
+#define XLA_PJRT_PLUGIN_STABLEHLO_MLX_COMPILE_H_
+
+#include <vector>
+
+#include "mlx/mlx.h"
+#include "mlir/IR/Operation.h"
+
+#include "stablehlo/reference/Api.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "xla/pjrt/plugin/mlx/utils.h"
+
+namespace mx = mlx::core;
+
+namespace compile {
+namespace detail {
+std::vector<mx::array> cbrt(const std::vector<mx::array>& inputs);
+std::vector<mx::array> func(mlir::ModuleOp mod, mlir::func::FuncOp op,
+                            const std::vector<mx::array>& inputs,
+                            bool recursive_compile = true);
+}  // namespace detail
+MlxFunc dotGeneral(mlir::stablehlo::DotGeneralOp op);
+MlxFunc gather(mlir::stablehlo::GatherOp op);
+MlxFunc pad(mlir::stablehlo::PadOp op);
+MlxFunc scatter(mlir::stablehlo::ScatterOp op);
+MlxFunc func(mlir::ModuleOp mod, mlir::func::FuncOp op);
+}  // namespace compile
+
+#endif  // XLA_PJRT_PLUGIN_STABLEHLO_MLX_COMPILE_H_
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/device.cc b/xla/pjrt/plugin/mlx/device.cc
new file mode 100644
index 0000000000..6984da5b7f
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/device.cc
@@ -0,0 +1,213 @@
+/* Copyright 2024 The OpenXLA Authors.
+
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "mlx/mlx.h"
+
+#include "xla/pjrt/plugin/mlx/device.h"
+
+#include "absl/status/statusor.h"
+#include "absl/strings/str_format.h"
+#include "absl/strings/string_view.h"
+#include "absl/types/span.h"
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/pjrt/pjrt_common.h"
+#include "xla/pjrt/pjrt_compiler.h"
+#include "xla/pjrt/pjrt_device_description.h"
+#include "xla/pjrt/plugin/mlx/logging.h"
+#include "xla/util.h"
+
+#define UNIMPLEMENTED(name) \
+  xla::Unimplemented("StablehloMlxDevice::" #name " is not implemented")
+
+namespace mlir::stablehlo {
+
+using xla::LiteralSlice;
+using xla::MutableBorrowingLiteral;
+using xla::PjRtClient;
+using xla::PjRtDevice;
+using xla::PjRtDeviceAttribute;
+using xla::PjRtDeviceDescription;
+using xla::PjRtGlobalDeviceId;
+using xla::PjRtLocalDeviceId;
+using xla::PjRtLocalHardwareId;
+using xla::PjRtMemorySpace;
+using xla::ScopedAsyncTrackingEvent;
+using xla::Unimplemented;
+
+namespace mx = mlx::core;
+
+inline std::string mlxDeviceToString(const mx::Device& device) {
+  std::ostringstream os;
+  os << device;
+  return os.str();
+}
+// Devices need a device description.
+class StablehloMlxDeviceDescription final : public PjRtDeviceDescription {
+ public:
+  explicit StablehloMlxDeviceDescription(int process_id, int local_device_id)
+      : id_(local_device_id),
+        process_index_(process_id),
+        local_hardware_id_(local_device_id),
+        debug_string_(mlxDeviceToString(mx::default_device())),
+        to_string_(absl::StrFormat("%s(id=%d,pid=%d)", debug_string_,
+                                   id_.value(), process_index_)) {
+    // TRACE_ME_MEMBER;
+  }
+
+  int id() const override {
+    // TRACE_ME_MEMBER;
+    return id_.value();
+  }
+  int process_index() const override {
+    // TRACE_ME_MEMBER;
+    return process_index_;
+  }
+  int local_hardware_id() const {
+    // TRACE_ME_MEMBER;
+    return local_hardware_id_;
+  }
+
+  absl::string_view device_kind() const override {
+    // TRACE_ME_MEMBER;
+    return DebugString();
+  }
+
+  absl::string_view DebugString() const override {
+    // TRACE_ME_MEMBER;
+    return debug_string_;
+  }
+
+  absl::string_view ToString() const override {
+    // TRACE_ME_MEMBER;
+    return to_string_;
+  }
+
+  const absl::flat_hash_map<std::string, PjRtDeviceAttribute>& Attributes()
+      const override {
+    // TRACE_ME_MEMBER;
+    return attributes_;
+  }
+
+ private:
+  PjRtGlobalDeviceId id_;
+  int process_index_;
+  int local_hardware_id_;
+  std::string debug_string_;
+  std::string to_string_;
+  absl::flat_hash_map<std::string, PjRtDeviceAttribute> attributes_ = {};
+};
+
+// Clients need devices, and clients own the devices.
+class StablehloMlxDevice : public PjRtDevice {
+ public:
+  explicit StablehloMlxDevice(PjRtClient* client)
+      : PjRtDevice(), client_(client), description_(0, 0) {
+    // TRACE_ME_MEMBER;
+  }
+
+  absl::string_view DebugString() const override {
+    // TRACE_ME_MEMBER;
+    return "StablehloMlxDevice";
+  }
+
+  PjRtLocalDeviceId local_device_id() const override {
+    // TRACE_ME_MEMBER;
+    return PjRtLocalDeviceId(local_hardware_id().value());
+  }
+
+  PjRtLocalHardwareId local_hardware_id() const override {
+    // TRACE_ME_MEMBER;
+    return PjRtLocalHardwareId(description_.local_hardware_id());
+  }
+
+  PjRtClient* client() const override {
+    // TRACE_ME_MEMBER;
+    return client_;
+  }
+
+  bool IsAddressable() const override {
+    // TRACE_ME_MEMBER;
+    return process_index() == client()->process_index();
+  }
+
+  absl::Status TransferToInfeed(const LiteralSlice& literal) override {
+    // TRACE_ME_MEMBER;
+    return UNIMPLEMENTED(TransferToInfeed);
+  }
+
+  absl::Status TransferFromOutfeed(MutableBorrowingLiteral literal) override {
+    // TRACE_ME_MEMBER;
+    return UNIMPLEMENTED(TransferFromOutfeed);
+  }
+
+  void AttachDefaultMemorySpace(PjRtMemorySpace* memory_space) {
+    // TRACE_ME_MEMBER;
+    memory_space_ = memory_space;
+  }
+
+  absl::Span<PjRtMemorySpace* const> memory_spaces() const override {
+    // TRACE_ME_MEMBER;
+    return absl::MakeSpan(&memory_space_, 1);
+  }
+
+  absl::StatusOr<PjRtMemorySpace*> default_memory_space() const override {
+    // TRACE_ME_MEMBER;
+    if (!memory_space_)
+      return absl::InternalError("Plugin memory space unset.");
+
+    return memory_space_;
+  }
+
+  std::unique_ptr<ScopedAsyncTrackingEvent> CreateAsyncTrackingEvent(
+      absl::string_view description) const override {
+    // TRACE_ME_MEMBER;
+    LOG(FATAL) << "Plugin does not implement CreateAsyncTrackingEvent.";
+    return nullptr;
+  }
+
+  const PjRtDeviceDescription& description() const override {
+    // TRACE_ME_MEMBER;
+    return description_;
+  }
+
+ private:
+  PjRtClient* client_;
+  PjRtMemorySpace* memory_space_;  // unpinned memory owned by client
+  StablehloMlxDeviceDescription description_;
+};
+
+// Device Description
+std::unique_ptr<PjRtDeviceDescription> GetStablehloMlxDeviceDescription(
+    int process_id, int local_device_id) {
+  return std::make_unique<StablehloMlxDeviceDescription>(process_id,
+                                                         local_device_id);
+}
+
+// Mlx Device
+std::unique_ptr<PjRtDevice> GetStablehloMlxDevice(PjRtClient* client) {
+  return std::make_unique<StablehloMlxDevice>(client);
+}
+
+void AttachStablehloMlxMemorySpace(PjRtDevice* device,
+                                   PjRtMemorySpace* memory_space) {
+  auto stablehlo_device = dynamic_cast<StablehloMlxDevice*>(device);
+  if (stablehlo_device == nullptr) {
+    LOG(FATAL) << "Plugin cannot attach memory space to device of kind "
+               << device->device_kind();
+    return;
+  }
+  stablehlo_device->AttachDefaultMemorySpace(memory_space);
+}
+
+}  // namespace mlir::stablehlo
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/device.h b/xla/pjrt/plugin/mlx/device.h
new file mode 100644
index 0000000000..ef42968a3a
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/device.h
@@ -0,0 +1,36 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_PJRT_PLUGIN_STABLEHLO_MLX_DEVICE_H_
+#define XLA_PJRT_PLUGIN_STABLEHLO_MLX_DEVICE_H_
+
+#include <memory>
+
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/pjrt/pjrt_compiler.h"
+#include "xla/pjrt/pjrt_device_description.h"
+
+namespace mlir::stablehlo {
+
+// Device Description
+std::unique_ptr<xla::PjRtDeviceDescription> GetStablehloMlxDeviceDescription(
+    int process_id, int local_device_id);
+
+// MLX Device
+std::unique_ptr<xla::PjRtDevice> GetStablehloMlxDevice(xla::PjRtClient* client);
+
+void AttachStablehloMlxMemorySpace(xla::PjRtDevice* device,
+                                   xla::PjRtMemorySpace* memory_space);
+
+}  // namespace mlir::stablehlo
+
+#endif  // XLA_PJRT_PLUGIN_STABLEHLO_MLX_DEVICE_H_
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/executable.cc b/xla/pjrt/plugin/mlx/executable.cc
new file mode 100644
index 0000000000..ec612eca89
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/executable.cc
@@ -0,0 +1,507 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/pjrt/plugin/mlx/executable.h"
+#include <iostream>
+#include <algorithm>
+#include <memory>
+#include <optional>
+#include <utility>
+#include <cstddef>
+#include <functional>
+#include <unordered_map>
+#include <vector>
+#include <numeric>
+#include <string>
+#include <tuple>
+#include <type_traits>
+
+// TODO(@cryptodeal): might need to update `BUILD`
+#include "mlir/IR/Visitors.h"
+#include "mlx/mlx.h"
+#include "absl/log/log.h"
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "llvm/ADT/TypeSwitch.h"
+#include "mlir/Bytecode/BytecodeWriter.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/IR/AsmState.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/MLIRContext.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/IR/OwningOpRef.h"
+#include "mlir/Interfaces/DataLayoutInterfaces.h"
+#include "mlir/Pass/PassManager.h"
+#include "mlir/Support/LLVM.h"
+#include "stablehlo/reference/Api.h"
+#include "stablehlo/transforms/optimization/Passes.h"
+#include "xla/hlo/translate/stablehlo.h"
+#include "xla/mlir/utils/error_util.h"
+#include "xla/mlir/utils/type_util.h"
+#include "xla/mlir_hlo/mhlo/transforms/passes.h"
+#include "xla/mlir_hlo/stablehlo_ext/transforms/passes.h"
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/pjrt/pjrt_executable.h"
+#include "xla/pjrt/pjrt_future.h"
+#include "xla/pjrt/plugin/mlx/buffer.h"
+#include "xla/pjrt/plugin/mlx/compile.h"
+#include "xla/pjrt/plugin/mlx/logging.h"
+#include "xla/pjrt/plugin/mlx/utils.h"
+#include "xla/service/computation_placer.h"
+#include "tsl/platform/status.h"
+#include "tsl/platform/statusor.h"
+
+#define DEBUG_TYPE "stablehlo-pjrt"
+
+namespace mx = mlx::core;
+namespace mlir::stablehlo {
+
+#define UNIMPLEMENTED(name) \
+  xla::Unimplemented("MlxPjRtBuffer::" #name " is not implemented")
+
+using xla::DeviceAssignment;
+using xla::PjRtBuffer;
+using xla::PjRtClient;
+using xla::PjRtDevice;
+using xla::PjRtFuture;
+using xla::PjRtLoadedExecutable;
+using xla::PjRtMemorySpace;
+
+mlir::OwningOpRef<ModuleOp> cloneIntoContext(ModuleOp module,
+                                             MLIRContext& context) {
+  // Clone the module into the context. MHLO->StableHLO just in case.
+  PassManager pm(module->getContext());
+  pm.addPass(mhlo::createHloLegalizeToStablehloPass());
+  if (failed(pm.run(module))) {
+    LOG(ERROR) << "Failed to convert MHLO to StableHLO";
+    return nullptr;
+  }
+
+  std::string bytecode;
+  llvm::raw_string_ostream os(bytecode);
+  mlir::BytecodeWriterConfig config;
+  mlir::OwningOpRef<mlir::ModuleOp> cloned = module.clone();
+  if (mlir::failed(mlir::writeBytecodeToFile(*cloned, os, config))) {
+    LOG(ERROR) << "Failed to write bytecode to string\n";
+    return nullptr;
+  }
+  return *parseStablehloModule(bytecode, context);
+}
+
+LogicalResult decomposeChloToStablehlo(ModuleOp module) {
+  PassManager pm(module->getContext());
+  stablehlo_ext::createChloLegalizeToStablehloPipeline(pm);
+  if (failed(pm.run(module))) {
+    return module->emitError() << "Failed to recompose CHLO";
+  }
+  return success();
+}
+
+LogicalResult runHardwareIndependentOptimizations(ModuleOp module) {
+  PassManager pm(module->getContext());
+  pm.addNestedPass<func::FuncOp>(
+      stablehlo::createStablehloTargetIndependentOptimizationPass());
+  if (failed(pm.run(module))) {
+    return module->emitError()
+           << "Failed to run hardware independent optimizations";
+  }
+  return success();
+}
+
+absl::Status validateModule(ModuleOp mod) {
+  absl::Status status;
+  // TODO: some ops should have additional checks (e.g. `stablehlo::ReduceOp`,
+  // `stablehlo::SortOp`, `stablehlo::ScatterOp`, etc.) where we need additional
+  // checks at compile time to ensure that we're correctly mapping to the
+  // corresponding MLX op.
+  mod.walk([&status](Operation* op) {
+    // Series of checks to ensure we support the module
+    return llvm::TypeSwitch<Operation*, WalkResult>(op)
+        .Case<ModuleOp, func::ReturnOp, stablehlo::ReturnOp, func::CallOp,
+              func::FuncOp, stablehlo::ConstantOp, stablehlo::IotaOp,
+              stablehlo::AbsOp, stablehlo::CbrtOp, stablehlo::CeilOp,
+              stablehlo::ConvertOp, stablehlo::CosineOp, stablehlo::ExpOp,
+              stablehlo::Expm1Op, stablehlo::FloorOp, stablehlo::ImagOp,
+              stablehlo::IsFiniteOp, stablehlo::LogOp, stablehlo::Log1pOp,
+              stablehlo::LogisticOp, stablehlo::NotOp, stablehlo::NegOp,
+              stablehlo::RealOp, stablehlo::RoundNearestEvenOp,
+              stablehlo::RsqrtOp, stablehlo::SignOp, stablehlo::SineOp,
+              stablehlo::SqrtOp, stablehlo::TanOp, stablehlo::TanhOp,
+              stablehlo::AddOp, stablehlo::Atan2Op, stablehlo::DivOp,
+              stablehlo::MaxOp, stablehlo::MinOp, stablehlo::MulOp,
+              stablehlo::PowOp, stablehlo::RemOp, stablehlo::ShiftLeftOp,
+              stablehlo::ShiftRightArithmeticOp, stablehlo::ShiftRightLogicalOp,
+              stablehlo::SubtractOp, stablehlo::AndOp, stablehlo::OrOp,
+              stablehlo::XorOp, stablehlo::ReduceOp, stablehlo::CompareOp,
+              stablehlo::SliceOp, stablehlo::DynamicSliceOp,
+              stablehlo::DynamicUpdateSliceOp, stablehlo::BitcastConvertOp,
+              stablehlo::BroadcastInDimOp, stablehlo::ConcatenateOp,
+              stablehlo::DotGeneralOp, stablehlo::GatherOp,
+              stablehlo::GetDimensionSizeOp, stablehlo::ReshapeOp,
+              stablehlo::ScatterOp, stablehlo::SelectOp, stablehlo::SortOp,
+              stablehlo::PadOp, stablehlo::TransposeOp>([&status, &op](auto o) {
+          // Validate all input types are compatible with mlx
+          for (auto operand : o->getOperands()) {
+            auto primitive_type = xla::ConvertMlirTypeToPrimitiveType(
+                mlir::cast<ShapedType>(operand.getType()).getElementType());
+            switch (primitive_type) {
+              case xla::PrimitiveType::PRED:
+              case xla::PrimitiveType::U8:
+              case xla::PrimitiveType::S8:
+              case xla::PrimitiveType::U16:
+              case xla::PrimitiveType::S16:
+              case xla::PrimitiveType::U32:
+              case xla::PrimitiveType::S32:
+              case xla::PrimitiveType::U64:
+              case xla::PrimitiveType::S64:
+              case xla::PrimitiveType::F16:
+              case xla::PrimitiveType::BF16:
+              case xla::PrimitiveType::F32:
+              case xla::PrimitiveType::C64:
+                break;
+              default: {
+                std::cout << "Unsupported type: "
+                          << xla::PrimitiveType_Name(primitive_type).c_str()
+                          << std::endl;
+                status = absl::UnimplementedError(
+                    absl::StrCat("Unsupported type: ", ToString(op)));
+                return WalkResult::interrupt();
+              }
+            }
+          }
+
+          // Validate all result types are compatible with mlx
+          for (auto result : o->getResults()) {
+            auto primitive_type = xla::ConvertMlirTypeToPrimitiveType(
+                mlir::cast<ShapedType>(result.getType()).getElementType());
+            switch (primitive_type) {
+              case xla::PrimitiveType::PRED:
+              case xla::PrimitiveType::U8:
+              case xla::PrimitiveType::S8:
+              case xla::PrimitiveType::U16:
+              case xla::PrimitiveType::S16:
+              case xla::PrimitiveType::U32:
+              case xla::PrimitiveType::S32:
+              case xla::PrimitiveType::U64:
+              case xla::PrimitiveType::S64:
+              case xla::PrimitiveType::F16:
+              case xla::PrimitiveType::BF16:
+              case xla::PrimitiveType::F32:
+              case xla::PrimitiveType::C64:
+                break;
+              default: {
+                std::cout << "Unsupported type: "
+                          << xla::PrimitiveType_Name(primitive_type).c_str()
+                          << std::endl;
+                status = absl::UnimplementedError(
+                    absl::StrCat("Unsupported type: ", ToString(op)));
+                return WalkResult::interrupt();
+              }
+            }
+          }
+          return mlir::WalkResult::advance();
+        })
+        // TODO(@cryptodeal): These ops were supported prior to `mlx::compile`
+        // need to modify the implementation to ensure no use of `mlx::eval` and
+        // ensure that `mlx::random::key` is being propogated correctly.
+        .Default([&status, &op](auto o) {
+          status = absl::UnimplementedError(
+              absl::StrCat("Unsupported op: ", ToString(op)));
+          return WalkResult::interrupt();
+        });
+  });
+  return status;
+}
+
+class MlirLoadedExecutable : public PjRtLoadedExecutable {
+ public:
+  MlirLoadedExecutable(ModuleOp module, DeviceAssignment assignment,
+                       absl::Span<PjRtDevice* const> devices,
+                       PjRtClient* client)
+      : PjRtLoadedExecutable(),
+        name_("MlirLoadedExecutable"),
+        assignment_(assignment),
+        devices_(client->devices()),
+        client_(client),
+        context_(),
+        module_(cloneIntoContext(module, context_)) {
+    // TRACE_ME_MEMBER;
+
+    auto main = module.lookupSymbol<mlir::func::FuncOp>("main");
+    // std::cout << "Compiling module: " << std::to_string(getHashValue(main))
+    //           << std::endl
+    //           << std::endl;
+    // std::cout << ToString(module_.get()).c_str() << std::endl << std::endl;
+    compiled_module_ = compile::func(module_.get(), main);
+
+    auto& main_block = main.front();
+    // force compilation by running w zeroed inputs
+    std::vector<mx::array> tmp_inputs;
+    for (auto arg : main_block.getArguments()) {
+      auto shaped_type = mlir::cast<ShapedType>(arg.getType());
+      tmp_inputs.push_back(
+          mx::zeros(std::vector<int32_t>(shaped_type.getShape().begin(),
+                                         shaped_type.getShape().end()),
+                    *utils::dtype::fromMlirType(shaped_type.getElementType())));
+    }
+    compiled_module_(tmp_inputs);
+  }
+
+  static absl::StatusOr<std::unique_ptr<PjRtLoadedExecutable>> CompileAndLoad(
+      ModuleOp module, DeviceAssignment assignment,
+      absl::Span<PjRtDevice* const> devices, PjRtClient* client) {
+    TRACE_ME;
+
+    mlir::BaseScopedDiagnosticHandler diagnostic_handler(module->getContext());
+    if (failed(decomposeChloToStablehlo(module))) {
+      return diagnostic_handler.ConsumeStatus();
+    }
+
+    // Simplify the graph using available HWI passes.
+    if (failed(runHardwareIndependentOptimizations(module))) {
+      return diagnostic_handler.ConsumeStatus();
+    }
+
+    auto module_status = validateModule(module);
+    if (!module_status.ok()) {
+      return module_status;
+    }
+
+    // auto device_info = mx::metal::device_info();
+    // for (const auto& [key, value] : device_info) {
+    //   std::cout << key << ": ";
+    //   std::visit([](const auto& v) { std::cout << v; }, value);
+    //   std::cout << std::endl;
+    // }
+
+    auto executable = std::make_unique<MlirLoadedExecutable>(module, assignment,
+                                                             devices, client);
+
+    return executable;
+  }
+
+  PjRtClient* client() const override {
+    // TRACE_ME_MEMBER;
+    return client_;
+  }
+
+  const DeviceAssignment& device_assignment() const override {
+    // TRACE_ME_MEMBER;
+    return assignment_;
+  }
+
+  absl::Span<const PjRtLoadedExecutable::LogicalDeviceIds>
+  addressable_device_logical_ids() const override {
+    // TRACE_ME_MEMBER;
+    LOG_UNIMPLEMENTED(addressable_device_logical_ids);
+    return {};
+  }
+
+  absl::Span<PjRtDevice* const> addressable_devices() const override {
+    // TRACE_ME_MEMBER;
+    return devices_;
+  }
+
+  // Helper function to get default mem from device.
+  PjRtMemorySpace* get_default_memory_space() const {
+    // TRACE_ME_MEMBER;
+    return devices_[0]->default_memory_space().value_or(nullptr);
+  }
+
+  absl::StatusOr<PjRtLoadedExecutable::Result> ExecuteWithMlxInterpreter(
+      absl::Span<PjRtBuffer* const> argument_handles, ModuleOp module,
+      PjRtDevice* device, bool fill_future) {
+    // TRACE_ME_MEMBER;
+
+    std::vector<mx::array> inputs;
+    for (auto& buffer : argument_handles) {
+      TF_ASSIGN_OR_RETURN(mx::array input, GetArrayFromBuffer(buffer));
+      inputs.push_back(std::move(input));
+    }
+    auto mlx_res = compiled_module_(inputs);
+    for (auto i = 0; i < mlx_res.size(); ++i) {
+      mlx_res[i] = mx::contiguous(mlx_res[i]);
+    }
+    mx::eval(mlx_res);
+
+    // Naive memory space selection, only using CPU global memory.
+    PjRtMemorySpace* memory_space =
+        device->default_memory_space().value_or(nullptr);
+
+    std::vector<std::unique_ptr<PjRtBuffer>> buffer_results(mlx_res.size());
+    for (auto i = 0; i < mlx_res.size(); i++) {
+      buffer_results[i] =
+          CreateMlirBufferFromMlxArray(mlx_res[i], memory_space);
+    }
+
+    std::optional<PjRtFuture<>> future;
+    if (fill_future) {
+      // Synchronous! To make async, this would need to return a future that
+      // is ready when the computation is done.
+      future = PjRtFuture<>(absl::OkStatus());
+    }
+
+    return PjRtLoadedExecutable::Result{future, std::move(buffer_results)};
+  }
+
+  absl::StatusOr<std::vector<std::vector<std::unique_ptr<PjRtBuffer>>>> Execute(
+      absl::Span<const std::vector<PjRtBuffer*>> argument_handles,
+      const xla::ExecuteOptions& options,
+      std::optional<std::vector<PjRtFuture<>>>& returned_futures) override {
+    // TRACE_ME_MEMBER;
+    if (argument_handles.size() != 1) {
+      // One arg handle per device.
+      return absl::InvalidArgumentError(
+          "MlirLoadedExecutable::Execute only supports a single argument "
+          "vector");
+    }
+
+    // Single device, synchronous, can always use 0.
+    PjRtDevice* device = devices_[0];
+    bool fill_future = returned_futures.has_value();
+    TF_ASSIGN_OR_RETURN(
+        PjRtLoadedExecutable::Result result,
+        ExecuteWithMlxInterpreter(argument_handles[0], module_.get(), device,
+                                  fill_future));
+    std::vector<std::vector<std::unique_ptr<PjRtBuffer>>> results;
+    results.push_back(std::move(result.buffers));
+    if (returned_futures.has_value()) {
+      returned_futures->push_back(std::move(result.future.value()));
+    }
+    return results;
+  }
+
+  absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>> ExecuteSharded(
+      absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,
+      const xla::ExecuteOptions& options,
+      std::optional<PjRtFuture<>>& returned_future, bool fill_future) override {
+    // TRACE_ME_MEMBER;
+    // Synchronous! To make async, have the device make a buffer with a ready
+    // future that is ready when the computation is done / buffer is ready.
+    TF_ASSIGN_OR_RETURN(
+        PjRtLoadedExecutable::Result result,
+        ExecuteWithMlxInterpreter(argument_handles, module_.get(), device,
+                                  fill_future));
+    if (returned_future.has_value() && fill_future) {
+      returned_future = std::move(result.future);
+    }
+    return std::move(result.buffers);
+  }
+
+  absl::StatusOr<std::vector<std::unique_ptr<PjRtBuffer>>> ExecutePortable(
+      absl::Span<PjRtBuffer* const> argument_handles, PjRtDevice* device,
+      const xla::ExecuteOptions& options,
+      std::optional<PjRtFuture<>>& returned_future, bool fill_future) override {
+    // TRACE_ME_MEMBER;
+    // Synchronous! To make async, have the device make a buffer with a ready
+    // future that is ready when the computation is done / buffer is ready.
+    TF_ASSIGN_OR_RETURN(
+        PjRtLoadedExecutable::Result result,
+        ExecuteWithMlxInterpreter(argument_handles, module_.get(), device,
+                                  fill_future));
+    if (returned_future.has_value() && fill_future) {
+      returned_future = std::move(result.future);
+    }
+    return std::move(result.buffers);
+  }
+
+  // TODO (@cryptodeal): Implement `SerializeExecutable`
+  // absl::StatusOr<std::string> SerializeExecutable() {}
+
+  void Delete() override {
+    // TRACE_ME_MEMBER;
+    module_.release();
+    module_ = nullptr;
+  }
+
+  bool IsDeleted() override {
+    // TRACE_ME_MEMBER;
+    return !module_;
+  }
+
+  // PjRtExecutable API.
+  int num_replicas() const override {
+    // TRACE_ME_MEMBER;
+    return assignment_.replica_count();
+  }
+
+  int num_partitions() const override {
+    // TRACE_ME_MEMBER;
+    return assignment_.computation_count();
+  }
+
+  int64_t SizeOfGeneratedCodeInBytes() const override {
+    // No generated code.. so just return 1.
+    // TRACE_ME_MEMBER;
+    return 1;
+  }
+
+  absl::string_view name() const override {
+    // TRACE_ME_MEMBER;
+    return name_;
+  }
+
+  absl::StatusOr<std::vector<std::shared_ptr<xla::HloModule>>> GetHloModules()
+      const override {
+    // TODO: This shouldn't be needed for an MLIR plugin, its only used in the
+    // JAX layer for determining output sharding, which exists on the mlir
+    // module.
+    // TRACE_ME_MEMBER;
+    auto moduleClone = llvm::cast<ModuleOp>(module_.get()->clone());
+    TF_ASSIGN_OR_RETURN(auto hlo_module,
+                        xla::ConvertStablehloToHlo(moduleClone));
+    return std::vector<std::shared_ptr<xla::HloModule>>{std::move(hlo_module)};
+  }
+
+  absl::StatusOr<std::vector<std::vector<absl::string_view>>>
+  GetOutputMemoryKinds() const override {
+    // TRACE_ME_MEMBER;
+    return UNIMPLEMENTED(GetOutputMemoryKinds);
+  }
+
+  absl::StatusOr<std::string> FingerprintExecutable() const override {
+    // TRACE_ME_MEMBER;
+    return UNIMPLEMENTED(FingerprintExecutable);
+  }
+
+ private:
+  std::string name_;
+  DeviceAssignment assignment_;
+  absl::Span<PjRtDevice* const> devices_;
+  PjRtClient* client_;
+  // MLX compilation context
+  MlxFunc compiled_module_;
+
+  // MLIR
+  MLIRContext context_;
+  mlir::OwningOpRef<ModuleOp> module_;
+};
+
+absl::StatusOr<std::unique_ptr<xla::PjRtLoadedExecutable>> StablehloMlxCompile(
+    mlir::ModuleOp module, DeviceAssignment assignment, PjRtClient* client) {
+  TRACE_ME;
+  return MlirLoadedExecutable::CompileAndLoad(module, assignment,
+                                              client->devices(), client);
+}
+
+absl::StatusOr<std::unique_ptr<xla::PjRtLoadedExecutable>> StablehloMlxCompile(
+    xla::XlaComputation const& computation, xla::DeviceAssignment assignment,
+    xla::PjRtClient* client) {
+  TRACE_ME;
+  MLIRContext context;
+  TF_ASSIGN_OR_RETURN(auto module,
+                      ConvertHloToStablehlo(context, &computation.proto()));
+  return StablehloMlxCompile(module.get(), assignment, client);
+}
+
+}  // namespace mlir::stablehlo
diff --git a/xla/pjrt/plugin/mlx/executable.h b/xla/pjrt/plugin/mlx/executable.h
new file mode 100644
index 0000000000..d970cde59d
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/executable.h
@@ -0,0 +1,33 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_PJRT_PLUGIN_STABLEHLO_MLX_EXECUTABLE_H_
+#define XLA_PJRT_PLUGIN_STABLEHLO_MLX_EXECUTABLE_H_
+#include <memory>
+
+#include "mlir/IR/BuiltinOps.h"
+#include "xla/hlo/builder/xla_computation.h"
+#include "xla/pjrt/pjrt_client.h"
+#include "xla/service/computation_placer.h"
+
+namespace mlir::stablehlo {
+
+absl::StatusOr<std::unique_ptr<xla::PjRtLoadedExecutable>> StablehloMlxCompile(
+    mlir::ModuleOp module, xla::DeviceAssignment assignment,
+    xla::PjRtClient* client);
+
+absl::StatusOr<std::unique_ptr<xla::PjRtLoadedExecutable>> StablehloMlxCompile(
+    xla::XlaComputation const& computation, xla::DeviceAssignment assignment,
+    xla::PjRtClient* client);
+
+}  // namespace mlir::stablehlo
+#endif  // XLA_PJRT_PLUGIN_STABLEHLO_MLX_EXECUTABLE_H_
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/logging.cc b/xla/pjrt/plugin/mlx/logging.cc
new file mode 100644
index 0000000000..3f21f67baf
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/logging.cc
@@ -0,0 +1,89 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/pjrt/plugin/mlx/logging.h"
+
+#include <cstdlib>
+
+#include "absl/base/log_severity.h"
+#include "absl/log/globals.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinAttributes.h"
+#include "mlir/IR/Operation.h"
+
+namespace mlir::stablehlo {
+
+std::string ToString(mlir::Attribute attr) {
+  std::string out;
+  llvm::raw_string_ostream os(out);
+  attr.print(os);
+  return out;
+}
+
+std::string ToString(SmallVector<DenseElementsAttr> attrs) {
+  std::string out;
+  llvm::raw_string_ostream os(out);
+  os << "[";
+  bool first = true;
+  for (auto attr : attrs) {
+    if (!first) os << ", ";
+    first = false;
+    attr.print(os);
+  }
+  os << "]";
+  return out;
+}
+std::string ToString(Operation& op) {
+  std::string out;
+  llvm::raw_string_ostream os(out);
+  op.print(os);
+  return out;
+}
+
+std::string ToString(Operation* op) {
+  std::string out;
+  llvm::raw_string_ostream os(out);
+  os << *op;
+  return out;
+}
+
+std::string ToString(mlir::Value value) {
+  std::string out;
+  llvm::raw_string_ostream os(out);
+  value.print(os);
+  return out;
+}
+
+std::string ToString(mlir::Block& block) {
+  std::string out;
+  llvm::raw_string_ostream os(out);
+  block.print(os);
+  return out;
+}
+
+void SetupLogLevelFromEnv() {
+  absl::SetMinLogLevel(absl::LogSeverityAtLeast::kError);
+  const char* log_env = std::getenv("PJRT_LOG_LEVEL");
+  if (!log_env) return;
+  if (strcmp(log_env, "INFO") == 0) {
+    absl::SetMinLogLevel(absl::LogSeverityAtLeast::kInfo);
+  } else if (strcmp(log_env, "WARNING") == 0) {
+    absl::SetMinLogLevel(absl::LogSeverityAtLeast::kWarning);
+  } else if (strcmp(log_env, "ERROR") == 0) {
+    absl::SetMinLogLevel(absl::LogSeverityAtLeast::kError);
+  } else {
+    LOG(ERROR) << "Invalid PJRT_LOG_LEVEL: " << log_env;
+  }
+}
+
+}  // namespace mlir::stablehlo
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/logging.h b/xla/pjrt/plugin/mlx/logging.h
new file mode 100644
index 0000000000..58f296ab2f
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/logging.h
@@ -0,0 +1,42 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_PJRT_PLUGIN_STABLEHLO_MLX_LOGGING_H_
+#define XLA_PJRT_PLUGIN_STABLEHLO_MLX_LOGGING_H_
+
+// This file has some joint logging to allow LOG and VLOG to play well with
+// MLIR data structures
+
+#include "absl/log/log.h"
+#include "mlir/IR/Attributes.h"
+#include "mlir/IR/BuiltinAttributes.h"
+
+#define LOG_UNIMPLEMENTED(name) \
+  LOG(ERROR) << "MlxPjRtBuffer::" #name " is not implemented"
+
+#define TRACE_ME LOG(INFO) << __func__ << "\n";
+
+#define TRACE_ME_MEMBER LOG(INFO) << __func__ << "(" << (void*)this << ")\n";
+
+namespace mlir::stablehlo {
+std::string ToString(mlir::Attribute attr);
+std::string ToString(llvm::SmallVector<mlir::DenseElementsAttr> attrs);
+std::string ToString(Operation* op);
+std::string ToString(Operation& op);
+std::string ToString(mlir::Value op);
+std::string ToString(mlir::Block& block);
+
+// Looks for `PJRT_LOG_LEVEL = INFO|WARNING|ERROR` in env variables.
+void SetupLogLevelFromEnv();
+}  // namespace mlir::stablehlo
+
+#endif  // XLA_PJRT_PLUGIN_STABLEHLO_MLX_LOGGING_H_
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/plugin_pjrt_test.cc b/xla/pjrt/plugin/mlx/plugin_pjrt_test.cc
new file mode 100644
index 0000000000..d56a22565f
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/plugin_pjrt_test.cc
@@ -0,0 +1,33 @@
+/* Copyright 2022 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/pjrt/c/pjrt_c_api_test.h"
+#include "xla/pjrt/c/pjrt_c_api_wrapper_impl.h"
+#include "xla/pjrt/pjrt_client_test.h"
+#include "xla/pjrt/plugin/mlx/client_c_pjrt.h"
+#include "xla/pjrt/plugin/mlx/client_cpp_pjrt.h"
+
+namespace pjrt {
+namespace {
+
+const bool kUnused =
+    (RegisterPjRtCApiTestFactory([]() { return GetPjrtApi(); },
+                                 /*platform_name=*/"stablehlo_mlx"),
+     true);
+
+const bool kUnused2 = (xla::RegisterTestClientFactory([]() {
+                         return mlir::stablehlo::CreateStablehloMlxPjrtClient();
+                       }),
+                       true);
+
+}  // namespace
+}  // namespace pjrt
\ No newline at end of file
diff --git a/xla/pjrt/plugin/mlx/utils.cc b/xla/pjrt/plugin/mlx/utils.cc
new file mode 100644
index 0000000000..384c5f1a4d
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/utils.cc
@@ -0,0 +1,632 @@
+#include <optional>
+#include <string>
+#include <tuple>
+#include <type_traits>
+#include <unordered_map>
+#include <vector>
+
+#include "llvm/ADT/Hashing.h"
+#include "mlir/IR/OperationSupport.h"
+#include "mlir/Support/LLVM.h"
+#include "xla/mlir/utils/type_util.h"
+#include "xla/pjrt/plugin/mlx/utils.h"
+#include "xla/pjrt/plugin/mlx/logging.h"
+#include "xla/shape_util.h"
+#include "xla/util.h"
+
+std::tuple<mx::Shape, mx::Shape, mx::Strides> shapeAndBytes(
+    absl::Span<int64_t const> dims,
+    std::optional<absl::Span<int64_t const>> byte_strides, mx::Dtype dtype) {
+  mx::Shape shape(dims.size());
+  mx::Strides strides;
+  mx::Shape raw_buffer_shape = {shape.size() && !byte_strides.has_value() ? 1
+                                                                          : 0};
+  for (auto i = 0; i < dims.size(); ++i) {
+    shape[i] = static_cast<int32_t>(dims[i]);
+    if (byte_strides.has_value()) {
+      auto stride_value = byte_strides.value()[i];
+      strides.push_back(stride_value / dtype.size());
+      if (strides[i] != 0) {
+        raw_buffer_shape[0] = std::max(
+            raw_buffer_shape[0], static_cast<int32_t>(stride_value) * shape[i]);
+      }
+    } else {
+      raw_buffer_shape[0] *= static_cast<int32_t>(dims[i]);
+    }
+  }
+  if (!byte_strides.has_value()) {
+    raw_buffer_shape[0] *= dtype.size();
+  } else if (raw_buffer_shape[0] == 0 and
+             std::find(dims.begin(), dims.end(), 0) == dims.end()) {
+    raw_buffer_shape[0] += dtype.size();
+  }
+  return std::make_tuple(shape, raw_buffer_shape, strides);
+}
+
+namespace utils {
+
+namespace dtype {
+xla::PrimitiveType asXlaPrimitiveType(mx::Dtype dtype) {
+  switch (dtype) {
+    case mx::bool_:
+      return xla::PrimitiveType::PRED;
+    case mx::uint8:
+      return xla::PrimitiveType::U8;
+    case mx::uint16:
+      return xla::PrimitiveType::U16;
+    case mx::uint32:
+      return xla::PrimitiveType::U32;
+    case mx::uint64:
+      return xla::PrimitiveType::U64;
+    case mx::int8:
+      return xla::PrimitiveType::S8;
+    case mx::int16:
+      return xla::PrimitiveType::S16;
+    case mx::int32:
+      return xla::PrimitiveType::S32;
+    case mx::int64:
+      return xla::PrimitiveType::S64;
+    case mx::float16:
+      return xla::PrimitiveType::F16;
+    case mx::float32:
+      return xla::PrimitiveType::F32;
+    case mx::bfloat16:
+      return xla::PrimitiveType::BF16;
+    case mx::complex64:
+      return xla::PrimitiveType::C64;
+  }
+}
+
+absl::StatusOr<mx::Dtype> fromXlaPrimitiveType(xla::PrimitiveType dtype) {
+  switch (dtype) {
+    case xla::PrimitiveType::PRED:
+      return mx::bool_;
+    case xla::PrimitiveType::BF16:
+      return mx::bfloat16;
+    case xla::PrimitiveType::F16:
+      return mx::float16;
+    case xla::PrimitiveType::F32:
+      return mx::float32;
+    case xla::PrimitiveType::S8:
+      return mx::int8;
+    case xla::PrimitiveType::S16:
+      return mx::int16;
+    case xla::PrimitiveType::S32:
+      return mx::int32;
+    case xla::PrimitiveType::S64:
+      return mx::int64;
+    case xla::PrimitiveType::U8:
+      return mx::uint8;
+    case xla::PrimitiveType::U16:
+      return mx::uint16;
+    case xla::PrimitiveType::U32:
+      return mx::uint32;
+    case xla::PrimitiveType::U64:
+      return mx::uint64;
+    case xla::PrimitiveType::C64:
+      return mx::complex64;
+    default:
+      return absl::InvalidArgumentError("Unsupported type");
+  }
+}
+
+absl::StatusOr<mx::Dtype> fromMlirType(mlir::Type type) {
+  auto primitive_type = xla::ConvertMlirTypeToPrimitiveType(type);
+  if (primitive_type == xla::PrimitiveType::PRIMITIVE_TYPE_INVALID) {
+    return xla::Internal("Unsupported type: %s",
+                         xla::PrimitiveType_Name(primitive_type));
+  }
+  return fromXlaPrimitiveType(primitive_type);
+}
+}  // namespace dtype
+
+namespace array {
+absl::StatusOr<mx::array> fromHostBuffer(
+    const void* data, absl::Span<int64_t const> dims,
+    std::optional<absl::Span<int64_t const>> byte_strides,
+    xla::PrimitiveType type) {
+  TF_ASSIGN_OR_RETURN(mx::Dtype dtype, dtype::fromXlaPrimitiveType(type));
+  auto [shape, raw_buffer_shape, strides] =
+      shapeAndBytes(dims, byte_strides, dtype);
+  mx::array typed_view =
+      mx::view(mx::array(reinterpret_cast<const uint8_t*>(data),
+                         raw_buffer_shape, mx::uint8),
+               dtype);
+  if (byte_strides.has_value()) {
+    auto res = mx::contiguous(mx::as_strided(typed_view, shape, strides, 0));
+    res.eval();
+    return res;
+  } else {
+    return mx::reshape(typed_view, shape);
+  }
+}
+
+absl::StatusOr<mx::array> fromHostLiteral(const xla::LiteralSlice& literal) {
+  return fromHostBuffer(literal.untyped_data(), literal.shape().dimensions(),
+                        std::nullopt, literal.shape().element_type());
+}
+
+template <typename T>
+absl::StatusOr<mx::array> fromDenseElementsAttr(mlir::DenseElementsAttr attr) {
+  auto attr_type = attr.getType();
+  // convert to mlx shape
+  std::vector<int32_t> shape(attr_type.getShape().begin(),
+                             attr_type.getShape().end());
+
+  // handle splat values
+  if (attr.isSplat()) {
+    if constexpr (std::is_same<T, xla::bfloat16>::value) {
+      return mx::full<mx::bfloat16_t>(
+          shape, static_cast<mx::bfloat16_t>(attr.getSplatValue<T>()));
+    } else if constexpr (std::is_same<T, xla::half>::value) {
+      return mx::full<mx::float16_t>(
+          shape, static_cast<mx::float16_t>(attr.getSplatValue<T>()));
+    } else if constexpr (std::is_same<T, std::complex<float>>::value) {
+      return mx::full<mx::complex64_t>(
+          shape, static_cast<mx::complex64_t>(attr.getSplatValue<T>()));
+    } else {
+      return mx::full<T>(shape, attr.getSplatValue<T>());
+    }
+  }
+
+  // handle non-splat values that are expanded to fit shape
+  auto it = attr.getValues<T>();
+  auto buffer = llvm::to_vector(it);
+  if (attr.size() != attr.getNumElements()) {
+    if constexpr (std::is_same<T, xla::bfloat16>::value) {
+      return mx::full(
+          shape,
+          mx::array(reinterpret_cast<const mx::bfloat16_t*>(buffer.data()),
+                    {static_cast<int32_t>(attr.size())}));
+    } else if constexpr (std::is_same<T, xla::half>::value) {
+      return mx::full(
+          shape,
+          mx::array(reinterpret_cast<const mx::float16_t*>(buffer.data()),
+                    {static_cast<int32_t>(attr.size())}));
+    } else if constexpr (std::is_same<T, std::complex<float>>::value) {
+      return mx::full(
+          shape,
+          mx::array(reinterpret_cast<const mx::complex64_t*>(buffer.data()),
+                    {static_cast<int32_t>(attr.size())}));
+    } else {
+      return mx::full(shape, mx::array(buffer.begin(),
+                                       {static_cast<int32_t>(attr.size())}));
+    }
+  }
+
+  if constexpr (std::is_same<T, xla::bfloat16>::value) {
+    return mx::array(reinterpret_cast<const mx::bfloat16_t*>(buffer.data()),
+                     shape);
+  } else if constexpr (std::is_same<T, xla::half>::value) {
+    return mx::array(reinterpret_cast<const mx::float16_t*>(buffer.data()),
+                     shape);
+  } else if constexpr (std::is_same<T, std::complex<float>>::value) {
+    return mx::array(reinterpret_cast<const mx::complex64_t*>(buffer.data()),
+                     shape);
+  } else {
+    return mx::array(buffer.begin(), shape);
+  }
+}
+
+absl::StatusOr<mx::array> fromDenseElementsAttr(mlir::DenseElementsAttr attr) {
+  auto element_type =
+      xla::ConvertMlirTypeToPrimitiveType(attr.getType().getElementType());
+  switch (element_type) {
+    case xla::PrimitiveType::PRED:
+      return fromDenseElementsAttr<bool>(attr);
+    case xla::PrimitiveType::U8:
+      return fromDenseElementsAttr<uint8_t>(attr);
+    case xla::PrimitiveType::S8:
+      return fromDenseElementsAttr<int8_t>(attr);
+    case xla::PrimitiveType::U16:
+      return fromDenseElementsAttr<uint16_t>(attr);
+    case xla::PrimitiveType::S16:
+      return fromDenseElementsAttr<int16_t>(attr);
+    case xla::PrimitiveType::U32:
+      return fromDenseElementsAttr<uint32_t>(attr);
+    case xla::PrimitiveType::S32:
+      return fromDenseElementsAttr<int32_t>(attr);
+    case xla::PrimitiveType::U64:
+      return fromDenseElementsAttr<uint64_t>(attr);
+    case xla::PrimitiveType::S64:
+      return fromDenseElementsAttr<int64_t>(attr);
+    case xla::PrimitiveType::F16:
+      return fromDenseElementsAttr<xla::half>(attr);
+    case xla::PrimitiveType::BF16:
+      return fromDenseElementsAttr<xla::bfloat16>(attr);
+    case xla::PrimitiveType::F32:
+      return fromDenseElementsAttr<float>(attr);
+    case xla::PrimitiveType::C64:
+      return fromDenseElementsAttr<std::complex<float>>(attr);
+    default:
+      return xla::Internal("Unsupported type: %s",
+                           xla::PrimitiveType_Name(element_type));
+  }
+}
+
+mx::array fromOperand(
+    const mlir::Value& operand,
+    const std::unordered_map<mlir::Operation*, std::vector<mx::array>>&
+        block_ctx,
+    const std::vector<mx::array>& inputs, bool debug) {
+  if (auto defining_op = operand.getDefiningOp(); defining_op) {
+    if (auto search = block_ctx.find(defining_op); search != block_ctx.end()) {
+      for (auto i = 0; i < defining_op->getNumResults(); i++) {
+        mlir::Value maybe_res = defining_op->getResult(i);
+        if (maybe_res == operand) {
+          if (debug)
+            std::cout << "Operator from block context @ idx:"
+                      << std::to_string(i) << std::endl
+                      << mlir::stablehlo::ToString(defining_op).c_str()
+                      << std::endl
+                      << std::endl;
+          return search->second[i];
+        }
+      }
+      if (debug)
+        std::cout << "Operator from block context not found:" << std::endl
+                  << mlir::stablehlo::ToString(defining_op).c_str() << std::endl
+                  << std::endl;
+    } else {
+      std::cout << "Defining op not found in block context: "
+                << mlir::stablehlo::ToString(defining_op).c_str() << std::endl;
+      return inputs[llvm::cast<mlir::BlockArgument>(operand).getArgNumber()];
+    }
+  }
+  auto block_arg = llvm::cast<mlir::BlockArgument>(operand);
+  if (debug)
+    std::cout << "Operator from block argument @ idx:"
+              << std::to_string(block_arg.getArgNumber()) << std::endl
+              << std::endl;
+  return inputs[block_arg.getArgNumber()];
+}
+
+}  // namespace array
+
+void printVector(const std::string& name, const std::vector<int32_t>& vec,
+                 bool indent) {
+  if (indent) {
+    std::cout << "\t";
+  }
+  std::cout << name.c_str() << ": { ";
+  for (auto i = 0; i < vec.size(); i++) {
+    std::cout << std::to_string(vec[i]);
+    if (i < vec.size() - 1) {
+      std::cout << ", ";
+    }
+  }
+  std::cout << " }" << std::endl;
+}
+
+absl::StatusOr<ScatterType> getScatterType(mlir::Block& block) {
+  unsigned op_count = 0;
+  utils::ScatterType scatter_type = utils::ScatterType::Replace;
+  for (const auto& op : block.getOperations()) {
+    switch (op_count) {
+      case 0:
+        if (llvm::dyn_cast<mlir::stablehlo::ReturnOp>(op)) {
+          return scatter_type;
+        }
+        if (llvm::dyn_cast<mlir::stablehlo::AddOp>(op)) {
+          scatter_type = ScatterType::Add;
+        } else if (llvm::dyn_cast<mlir::stablehlo::MulOp>(op)) {
+          scatter_type = ScatterType::Prod;
+        } else if (llvm::dyn_cast<mlir::stablehlo::MulOp>(op)) {
+          scatter_type = ScatterType::Prod;
+        } else if (llvm::dyn_cast<mlir::stablehlo::MaxOp>(op)) {
+          scatter_type = ScatterType::Max;
+        } else if (llvm::dyn_cast<mlir::stablehlo::MinOp>(op)) {
+          scatter_type = ScatterType::Min;
+        } else {
+          return xla::Internal("Unsupported comparator: %s",
+                               mlir::stablehlo::ToString(block));
+        }
+        break;
+      case 1:
+        if (llvm::dyn_cast<mlir::stablehlo::ReturnOp>(op)) {
+          return scatter_type;
+        }
+      default:
+        return xla::Internal("Unsupported comparator: %s",
+                             mlir::stablehlo::ToString(block));
+    }
+  }
+}
+
+absl::StatusOr<std::pair<unsigned, mx::ComparatorType>> getSortInfo(
+    mlir::Block& block) {
+  unsigned op_count = 0;
+  mx::ComparatorType comparator = mx::ComparatorType::LessThan;
+  unsigned input_idx = 0;
+  for (auto& op : block.getOperations()) {
+    switch (op_count) {
+      case 0: {
+        if (auto compare_op = llvm::dyn_cast<mlir::stablehlo::CompareOp>(op)) {
+          switch (compare_op.getComparisonDirection()) {
+            case mlir::stablehlo::ComparisonDirection::GT: {
+              auto lhs =
+                  llvm::dyn_cast<mlir::BlockArgument>(compare_op.getLhs());
+              auto rhs =
+                  llvm::dyn_cast<mlir::BlockArgument>(compare_op.getRhs());
+              if (lhs && rhs) {
+                unsigned lhs_idx = static_cast<unsigned>(lhs.getArgNumber());
+                unsigned rhs_idx = static_cast<unsigned>(rhs.getArgNumber());
+                if (rhs_idx - 1 == lhs_idx) {
+                  comparator = mx::ComparatorType::GreaterThan;
+                  input_idx = lhs_idx == 0 ? 0 : lhs_idx / 2;
+                  break;
+                }
+              }
+              return xla::Internal("Unsupported comparator: %s",
+                                   mlir::stablehlo::ToString(block));
+            }
+            case mlir::stablehlo::ComparisonDirection::LT: {
+              auto lhs =
+                  llvm::dyn_cast<mlir::BlockArgument>(compare_op.getLhs());
+              auto rhs =
+                  llvm::dyn_cast<mlir::BlockArgument>(compare_op.getRhs());
+              if (lhs && rhs) {
+                unsigned lhs_idx = static_cast<unsigned>(lhs.getArgNumber());
+                unsigned rhs_idx = static_cast<unsigned>(rhs.getArgNumber());
+                if (rhs_idx - 1 == lhs_idx) {
+                  comparator = mx::ComparatorType::LessThan;
+                  input_idx = lhs_idx == 0 ? 0 : lhs_idx / 2;
+                  break;
+                }
+              }
+              return xla::Internal("Unsupported comparator: %s",
+                                   mlir::stablehlo::ToString(block));
+            }
+            default:
+              return xla::Internal("Unsupported comparator: %s",
+                                   mlir::stablehlo::ToString(block));
+          }
+          break;
+        }
+      }
+      case 1:
+        if (llvm::dyn_cast<mlir::stablehlo::ReturnOp>(op)) break;
+      default:
+        return xla::Internal("Unsupported comparator: %s",
+                             mlir::stablehlo::ToString(block));
+    }
+    op_count++;
+  }
+  return std::make_pair(input_idx, comparator);
+}
+
+// Returns true if the given `ReduceOp` matches
+// `zml.Tensor.argMax` op implementation.
+bool isArgMaxReduce(mlir::stablehlo::ReduceOp reduce_op) {
+  mlir::Block& body_block = reduce_op.getBody().front();
+  unsigned op_count = 0;
+  // TODO (@cryptodeal): This is a lazy verification; maybe check operands
+  // for each operation in the block.
+  for (auto& op : body_block.getOperations()) {
+    switch (op_count) {
+      case 0: {
+        auto compare_op = llvm::dyn_cast<mlir::stablehlo::CompareOp>(op);
+        if (compare_op && compare_op.getComparisonDirection() ==
+                              mlir::stablehlo::ComparisonDirection::GT) {
+          break;
+        } else
+          return false;
+      }
+      case 1: {
+        auto compare_op = llvm::dyn_cast<mlir::stablehlo::CompareOp>(op);
+        if (compare_op && compare_op.getComparisonDirection() ==
+                              mlir::stablehlo::ComparisonDirection::NE) {
+          break;
+        } else
+          return false;
+      }
+      case 2:
+        if (llvm::dyn_cast<mlir::stablehlo::OrOp>(op)) {
+          break;
+        } else
+          return false;
+      case 3:
+        if (llvm::dyn_cast<mlir::stablehlo::SelectOp>(op)) {
+          break;
+        } else
+          return false;
+      case 4: {
+        auto compare_op = llvm::dyn_cast<mlir::stablehlo::CompareOp>(op);
+        if (compare_op && compare_op.getComparisonDirection() ==
+                              mlir::stablehlo::ComparisonDirection::EQ) {
+          break;
+        } else
+          return false;
+      }
+      case 5: {
+        auto compare_op = llvm::dyn_cast<mlir::stablehlo::CompareOp>(op);
+        if (compare_op && compare_op.getComparisonDirection() ==
+                              mlir::stablehlo::ComparisonDirection::LT) {
+          break;
+        } else
+          return false;
+      }
+      case 6:
+        if (llvm::dyn_cast<mlir::stablehlo::AndOp>(op)) {
+          break;
+        } else
+          return false;
+      case 7:
+        if (llvm::dyn_cast<mlir::stablehlo::OrOp>(op)) {
+          break;
+        } else
+          return false;
+      case 8:
+        if (llvm::dyn_cast<mlir::stablehlo::SelectOp>(op)) {
+          break;
+        } else
+          return false;
+      case 9:
+        if (llvm::dyn_cast<mlir::stablehlo::ReturnOp>(op)) {
+          break;
+        } else
+          return false;
+      default:
+        return false;
+    }
+    op_count++;
+  }
+  return true;
+}
+
+// Returns true if the given `ReduceOp` matches
+// `zml.Tensor.sum` op implementation.
+bool isSumReduce(mlir::stablehlo::ReduceOp reduce_op) {
+  mlir::Block& body_block = reduce_op.getBody().front();
+  unsigned op_count = 0;
+  // TODO (@cryptodeal): This is a lazy verification; maybe check operands
+  // for each operation in the block.
+  for (auto& op : body_block.getOperations()) {
+    switch (op_count) {
+      case 0:
+        if (llvm::dyn_cast<mlir::stablehlo::AddOp>(op)) {
+          break;
+        } else
+          return false;
+      case 1:
+        if (llvm::dyn_cast<mlir::stablehlo::ReturnOp>(op)) {
+          break;
+        } else
+          return false;
+    }
+    op_count++;
+  }
+  return true;
+}
+
+// Returns true if the given `ReduceOp` matches
+// `zml.Tensor.max` op implementation.
+bool isMaxReduce(mlir::stablehlo::ReduceOp reduce_op) {
+  mlir::Block& body_block = reduce_op.getBody().front();
+  unsigned op_count = 0;
+  // TODO (@cryptodeal): This is a lazy verification; maybe check operands
+  // for each operation in the block.
+  for (auto& op : body_block.getOperations()) {
+    switch (op_count) {
+      case 0:
+        if (llvm::dyn_cast<mlir::stablehlo::MaxOp>(op)) {
+          break;
+        } else
+          return false;
+      case 1:
+        if (llvm::dyn_cast<mlir::stablehlo::ReturnOp>(op)) {
+          break;
+        } else
+          return false;
+    }
+    op_count++;
+  }
+  return true;
+}
+
+// Returns true if the given `ReduceOp` matches
+// `zml.Tensor.max` op implementation.
+bool isMinReduce(mlir::stablehlo::ReduceOp reduce_op) {
+  mlir::Block& body_block = reduce_op.getBody().front();
+  unsigned op_count = 0;
+  // TODO (@cryptodeal): This is a lazy verification; maybe check operands
+  // for each operation in the block.
+  for (auto& op : body_block.getOperations()) {
+    switch (op_count) {
+      case 0:
+        if (llvm::dyn_cast<mlir::stablehlo::MinOp>(op)) {
+          break;
+        } else
+          return false;
+      case 1:
+        if (llvm::dyn_cast<mlir::stablehlo::ReturnOp>(op)) {
+          break;
+        } else
+          return false;
+    }
+    op_count++;
+  }
+  return true;
+}
+
+// Returns true if the given `ReduceOp` matches
+// `zml.Tensor.any` op implementation.
+bool isAnyReduce(mlir::stablehlo::ReduceOp reduce_op) {
+  mlir::Block& body_block = reduce_op.getBody().front();
+  unsigned op_count = 0;
+  // TODO (@cryptodeal): This is a lazy verification; maybe check operands
+  // for each operation in the block.
+  for (auto& op : body_block.getOperations()) {
+    switch (op_count) {
+      case 0:
+        if (llvm::dyn_cast<mlir::stablehlo::OrOp>(op)) {
+          break;
+        } else
+          return false;
+      case 1:
+        if (llvm::dyn_cast<mlir::stablehlo::ReturnOp>(op)) {
+          break;
+        } else
+          return false;
+    }
+    op_count++;
+  }
+  return true;
+}
+
+llvm::hash_code computeHash(
+    mlir::Operation* op,
+    llvm::function_ref<llvm::hash_code(mlir::Value)> hashOperands,
+    llvm::function_ref<llvm::hash_code(mlir::Value)> hashResults) {
+  // Hash operations based upon their:
+  //   - Operation Name
+  //   - Attributes
+  //   - Properties
+
+  // Potentially need further specialization for:
+  //   - stablehlo::ScatterOp (requires determining the scatter type)
+
+  llvm::hash_code hash = llvm::hash_combine(
+      op->getName(), op->getRawDictionaryAttrs(), op->hashProperties());
+
+  //   - Operands
+  if (op->hasTrait<mlir::OpTrait::IsCommutative>() &&
+      op->getNumOperands() > 0) {
+    size_t operandHash = hashOperands(op->getOperand(0));
+    for (auto operand : op->getOperands().drop_front())
+      operandHash += hashOperands(operand);
+    hash = llvm::hash_combine(hash, operandHash);
+  } else {
+    for (mlir::Value operand : op->getOperands())
+      hash = llvm::hash_combine(hash, hashOperands(operand));
+  }
+
+  //   - Results
+  for (mlir::Value result : op->getResults())
+    hash = llvm::hash_combine(hash, hashResults(result));
+
+  return hash;
+}
+
+uint64_t getHashValue(mlir::func::FuncOp func_op) {
+  llvm::hash_code hash = {};
+  for (mlir::NamedAttribute attr : func_op->getAttrs()) {
+    if (attr.getName() == func_op.getSymNameAttrName()) {
+      continue;
+    }
+    hash = llvm::hash_combine(hash, attr);
+  }
+  func_op.getBody().walk([&](mlir::Operation* op) {
+    hash = llvm::hash_combine(
+        hash, computeHash(op, mlir::OperationEquivalence::ignoreHashValue,
+                          mlir::OperationEquivalence::ignoreHashValue));
+  });
+  return hash;
+}
+
+uint64_t getHashValue(mlir::Operation* op) {
+  return computeHash(op, mlir::OperationEquivalence::ignoreHashValue,
+                     mlir::OperationEquivalence::ignoreHashValue);
+}
+
+}  // namespace utils
diff --git a/xla/pjrt/plugin/mlx/utils.h b/xla/pjrt/plugin/mlx/utils.h
new file mode 100644
index 0000000000..f19257b5a2
--- /dev/null
+++ b/xla/pjrt/plugin/mlx/utils.h
@@ -0,0 +1,94 @@
+/* Copyright 2024 The OpenXLA Authors.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_PJRT_PLUGIN_STABLEHLO_MLX_UTILS_H_
+#define XLA_PJRT_PLUGIN_STABLEHLO_MLX_UTILS_H_
+
+#include <optional>
+#include <string>
+#include <unordered_map>
+#include <vector>
+
+#include "absl/status/statusor.h"
+#include "absl/types/span.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "stablehlo/reference/Api.h"
+#include "mlir/IR/OwningOpRef.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/Operation.h"
+#include "mlir/IR/Value.h"
+#include "mlx/mlx.h"
+#include "xla/literal.h"
+
+namespace mx = mlx::core;
+
+using MlxFunc =
+    std::function<std::vector<mx::array>(const std::vector<mx::array>&)>;
+using OwnedMlxFunc = std::shared_ptr<MlxFunc>;
+
+namespace utils {
+
+struct FuncInfo {
+  mlir::OwningOpRef<mlir::ModuleOp> mod_;
+  std::string name_;
+
+  FuncInfo(mlir::ModuleOp mod, std::string name)
+      : mod_(mod.clone()), name_(name) {}
+};
+
+enum ScatterType {
+  Replace,
+  Add,
+  Prod,
+  Max,
+  Min,
+};
+namespace dtype {
+xla::PrimitiveType asXlaPrimitiveType(mx::Dtype dtype);
+
+absl::StatusOr<mx::Dtype> fromXlaPrimitiveType(xla::PrimitiveType dtype);
+
+absl::StatusOr<mx::Dtype> fromMlirType(mlir::Type type);
+
+}  // namespace dtype
+
+namespace array {
+absl::StatusOr<mx::array> fromHostBuffer(
+    const void* data, absl::Span<int64_t const> dims,
+    std::optional<absl::Span<int64_t const>> byte_strides,
+    xla::PrimitiveType type);
+
+absl::StatusOr<mx::array> fromHostLiteral(const xla::LiteralSlice& literal);
+
+absl::StatusOr<mx::array> fromDenseElementsAttr(mlir::DenseElementsAttr attr);
+mx::array fromOperand(
+    const mlir::Value& operand,
+    const std::unordered_map<mlir::Operation*, std::vector<mx::array>>&
+        block_ctx,
+    const std::vector<mx::array>& inputs, bool debug = false);
+}  // namespace array
+
+void printVector(const std::string& name, const std::vector<int32_t>& vec,
+                 bool indent = false);
+absl::StatusOr<ScatterType> getScatterType(mlir::Block& block);
+absl::StatusOr<std::pair<unsigned, mx::ComparatorType>> getSortInfo(
+    mlir::Block& block);
+bool isArgMaxReduce(mlir::stablehlo::ReduceOp reduce_op);
+bool isSumReduce(mlir::stablehlo::ReduceOp reduce_op);
+bool isMaxReduce(mlir::stablehlo::ReduceOp reduce_op);
+bool isMinReduce(mlir::stablehlo::ReduceOp reduce_op);
+bool isAnyReduce(mlir::stablehlo::ReduceOp reduce_op);
+uint64_t getHashValue(mlir::func::FuncOp func_op);
+uint64_t getHashValue(mlir::Operation* op);
+}  // namespace utils
+
+#endif  // XLA_PJRT_PLUGIN_STABLEHLO_MLX_UTILS_H_
\ No newline at end of file
